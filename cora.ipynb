{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from superneuromat.neuromorphicmodel import NeuromorphicModel\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from dataclasses import dataclass, field\n",
    "\n",
    "\n",
    "# from: https://stackoverflow.com/a/21894086/2712730\n",
    "class bidict(dict):\n",
    "    \"\"\"Creates a dictionary that supports reverse lookups via the .inverse attribute.\n",
    "\n",
    "    Args:\n",
    "        dict (dict): The original dictionary.\n",
    "\n",
    "    Properties:\n",
    "        inverse (dict): A dictionary that maps values to keys.\n",
    "    \"\"\"\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(bidict, self).__init__(*args, **kwargs)\n",
    "        self.inverse = {}\n",
    "        for key, value in self.items():\n",
    "            self.inverse.setdefault(value, []).append(key)\n",
    "\n",
    "    def __setitem__(self, key, value):\n",
    "        if key in self:\n",
    "            self.inverse[self[key]].remove(key)\n",
    "        super(bidict, self).__setitem__(key, value)\n",
    "        self.inverse.setdefault(value, []).append(key)\n",
    "\n",
    "    def __delitem__(self, key):\n",
    "        self.inverse.setdefault(self[key], []).remove(key)\n",
    "        if self[key] in self.inverse and not self.inverse[self[key]]:\n",
    "            del self.inverse[self[key]]\n",
    "        super(bidict, self).__delitem__(key)\n",
    "\n",
    "\n",
    "def train_test_split_indices(papers, test_size=0.2, rng=None) -> tuple[np.ndarray[int], np.ndarray[int]]:\n",
    "    \"\"\"\n",
    "    Splits a list of papers into train and test indices.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    papers : iterable of papers with ids\n",
    "        List or dict of papers to split.\n",
    "    test_size : float, optional\n",
    "        if < 1, proportion of papers to reserve for testing, by default 0.2\n",
    "        if > 1, number of papers to reserve for testing\n",
    "    rng : int, optional\n",
    "        Random state for the random number generator, default uses numpy's random\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    train_indices : list of int\n",
    "        List of indices for the training set.\n",
    "    test_indices : list of int\n",
    "        List of indices for the testing set.\n",
    "    \"\"\"\n",
    "    n = papers if isinstance(papers, int) else len(papers)\n",
    "    if test_size < 1:\n",
    "        test_size = int(np.floor(test_size * n))  # number of papers in test\n",
    "    if rng is None:\n",
    "        rng = np.random.default_rng()  # setup rng\n",
    "    elif isinstance(rng, int):\n",
    "        rng = np.random.default_rng(rng)\n",
    "\n",
    "    if isinstance(papers, int):\n",
    "        indices = np.arange(n)  # generate indices\n",
    "    elif isinstance(papers, (list, tuple)):\n",
    "        indices = papers  # assume papers is a list of indices\n",
    "    else:  # assume papers is a dict or mapping of papers with a .values() method\n",
    "        indices = [paper.idx for paper in papers.values()]  # grab indices from dict entries\n",
    "    indices = np.asarray(indices, dtype=int)\n",
    "\n",
    "    # shuffle and split\n",
    "    rng.shuffle(indices)\n",
    "    test_indices = indices[:test_size]\n",
    "    train_indices = indices[test_size:]\n",
    "    return train_indices, test_indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(2024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load CORA dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Case_Based',\n",
       " 'Genetic_Algorithms',\n",
       " 'Neural_Networks',\n",
       " 'Probabilistic_Methods',\n",
       " 'Reinforcement_Learning',\n",
       " 'Rule_Learning',\n",
       " 'Theory'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@dataclass\n",
    "class Paper:\n",
    "    idx: int  # Paper ID\n",
    "    label: str  # Paper category/topic\n",
    "    features: tuple[bool, ...] = ()  # binary features\n",
    "    citations: list[int] = field(default_factory=list)  # IDs of papers cited by this paper\n",
    "\n",
    "papers = {}\n",
    "\n",
    "# Load in training data\n",
    "content = pd.read_csv(\"data/Cora/cora/cora.content\", sep=\"\\t\", header=None)\n",
    "citations = pd.read_csv(\"data/Cora/cora/cora.cites\", sep=\"\\t\", header=None)\n",
    "\n",
    "labels = set()  # set of unique labels\n",
    "\n",
    "for paper in content.itertuples(index=False):  # create papers from data\n",
    "    idx = paper[0]\n",
    "    features = tuple([int(feature) for feature in paper[1:-1]])  # parse features\n",
    "    papers[idx] = Paper(idx, paper[-1], features)  # create paper object\n",
    "    labels.add(paper[-1])  # label is the last column. add to set of labels\n",
    "\n",
    "for paper_idx, citation in citations.itertuples(index=False):\n",
    "    papers[paper_idx].citations.append(citation)  # parse citations\n",
    "\n",
    "labels  # show labels set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create train / test split  \n",
    "Create two lists of IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = 0.2\n",
    "train_idxs, test_idxs = train_test_split_indices(papers, test_size=test_size, rng=rng)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize our model\n",
    "model = NeuromorphicModel()\n",
    "\n",
    "# Create our output neurons, set threshold very high so that we control when they spike during training.\n",
    "# dict mapping {category: neuron_id}\n",
    "lbl_threshold = 2.0\n",
    "strong_connection = 5.0\n",
    "weak_connection = 1.0\n",
    "unknown_connection = 0.00001\n",
    "\n",
    "lbl_neurons = bidict({label: model.create_neuron(threshold=lbl_threshold).idx for label in labels})\n",
    "\n",
    "# Create our input neurons, one for each pixel of the image resolution.\n",
    "paper_neurons = {}  # dict mapping {paper_id: neuron_id}\n",
    "\n",
    "# make a neuron for each paper\n",
    "for paper in papers.values():\n",
    "    paper_neurons[paper.idx] = neuron_id = model.create_neuron().idx\n",
    "\n",
    "    if paper.idx in train_idxs:\n",
    "        # Make an explicitly STRONG synapse connecting the input to the output\n",
    "        output_id = lbl_neurons[paper.label]  # (training paper to topic)\n",
    "        model.create_synapse(neuron_id, output_id, weight=strong_connection, stdp_enabled=False, delay=1)\n",
    "        # model.create_synapse(output_id, neuron_id, weight=strong_connection, stdp_enabled=False, delay=1)\n",
    "    else:  # test set, connect this input neuron to all output neurons\n",
    "        # Connect our input neuron to output neurons (test paper to topic)\n",
    "        for output_id in lbl_neurons.values():\n",
    "            # Randomize initial weight\n",
    "            weight1 = (rng.uniform(-1, 1)) * unknown_connection\n",
    "            weight2 = (rng.uniform(-1, 1)) * unknown_connection\n",
    "            # Make a synapse connecting the input to the output\n",
    "            model.create_synapse(neuron_id, output_id, weight=weight1, stdp_enabled=True, delay=1)\n",
    "            # model.create_synapse(output_id, neuron_id, weight=weight2, stdp_enabled=True, delay=1)\n",
    "\n",
    "# connect papers by their citations (paper to paper connections)\n",
    "for paper in papers.values():\n",
    "    for citation in paper.citations:\n",
    "        model.create_synapse(paper_neurons[paper.idx], paper_neurons[citation], weight=weak_connection, stdp_enabled=True, delay=1)\n",
    "        model.create_synapse(paper_neurons[citation], paper_neurons[paper.idx], weight=weak_connection, stdp_enabled=True, delay=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### loop through our dataset and add spikes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestep = 0\n",
    "train_idxs_augmented = np.append(train_idxs, train_idxs)\n",
    "for idx in train_idxs_augmented:\n",
    "    paper = papers[idx]  # get paper by id\n",
    "    # Add a spike that will exceed the threshold for the respective label neuron\n",
    "    model.add_spike(timestep + 1, lbl_neurons[paper.label], strong_connection)\n",
    "    # Add spikes to the paper\n",
    "    model.add_spike(timestep, paper_neurons[paper.idx], strong_connection)\n",
    "    timestep += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup the model and perform a training pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your model has 2715 neurons and 16812 synapses.\n",
      "4334 time steps will be simulated.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab41d4a7e1cb4343a827d73c0a7ed6f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4334 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Set up our stdp, only one timestep because we only want it looking at the results\n",
    "# of what our input layer does to our output layer\n",
    "model.stdp_setup(time_steps=2, Apos=[1e-4, 1e-4], Aneg=[1e-5, 1e-5], negative_update=True, positive_update=True)\n",
    "\n",
    "print(f\"Your model has {model.num_neurons} neurons and {model.num_synapses} synapses.\\n{timestep} time steps will be simulated.\")\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Simulate\n",
    "with tqdm(total=timestep) as pbar:\n",
    "    model.simulate(time_steps=timestep, callback=lambda _s, _t, _n: pbar.update())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def blank_copy(model):\n",
    "    new = NeuromorphicModel()\n",
    "    new.neuron_states = model.neuron_states.copy()\n",
    "    new.neuron_refractory_periods_state = model.neuron_refractory_periods_state.copy()\n",
    "    new.neuron_thresholds = model.neuron_thresholds.copy()\n",
    "    new.neuron_leaks = model.neuron_leaks.copy()\n",
    "    new.neuron_reset_states = model.neuron_reset_states.copy()\n",
    "    new.neuron_refractory_periods = model.neuron_refractory_periods.copy()\n",
    "    new.pre_synaptic_neuron_ids = model.pre_synaptic_neuron_ids.copy()\n",
    "    new.post_synaptic_neuron_ids = model.post_synaptic_neuron_ids.copy()\n",
    "    new.synaptic_weights = model.synaptic_weights.copy()\n",
    "    new.synaptic_delays = model.synaptic_delays.copy()\n",
    "    new.enable_stdp = model.enable_stdp.copy()\n",
    "    return new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = model.synaptic_weights.copy()\n",
    "states = model.neuron_states.copy()\n",
    "model.stdp = False\n",
    "mcopy = blank_copy(model)\n",
    "\n",
    "\n",
    "def reset_model(model):\n",
    "    model.reset()\n",
    "    model.synaptic_weights = weights.copy()\n",
    "    model.neuron_states = states.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 10022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed: 10040\n",
      "Paper: 3235\tCategory: Probabilistic_Methods\n",
      "No category spiked\n",
      "==========\n",
      "3\t 11.731\tProbabilistic_Methods\n",
      "5\t 6.731\tCase_Based\n",
      "2\t 1.731\tNeural_Networks\n",
      "6\t 1.731\tGenetic_Algorithms\n",
      "4\t 1.731\tReinforcement_Learning\n",
      "0\t 0.000\tTheory\n",
      "1\t 0.000\tRule_Learning\n",
      "[0.0, 0.0, 1.7313816744817627, 11.731378683445362, 1.7313635441042492, 6.7313780613457475, 1.7313743553790844]\n",
      "[99, 99, 99, 99, 99, 99, 99]\n",
      "[0 0 0 0 0 0 0]\n",
      "[0 0 0 0 0 0 0]\n",
      "[0 0 0 0 0 0 0]\n",
      "[0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "seed += 1\n",
    "print(f\"Seed: {seed}\")\n",
    "rng = np.random.default_rng(seed)\n",
    "model2 = blank_copy(model)\n",
    "model2.stdp = False\n",
    "# model2 = model\n",
    "# reset_model(model2)\n",
    "# model2.enable_stdp = np.zeros(model.num_neurons)\n",
    "nlabels = len(labels)\n",
    "model2.neuron_thresholds[:nlabels] = [99] * nlabels\n",
    "\n",
    "# paper = random.choice(list(papers.values()))  # pick a random paper\n",
    "# paper = papers[random.choice(train_idxs)]  # pick a random paper from training set\n",
    "paper = papers[rng.choice(test_idxs)]  # pick a random paper from testing set\n",
    "\n",
    "print(f\"Paper: {paper.idx}\\tCategory: {paper.label}\")\n",
    "\n",
    "spike_n = 3\n",
    "\n",
    "for t in range(spike_n):\n",
    "    model2.add_spike(t, paper_neurons[paper.idx], 10001.)\n",
    "\n",
    "model2.simulate(spike_n + 1)\n",
    "\n",
    "spiked_ids = {idx: lbl_neurons.inverse[idx][0]\n",
    "              for idx, spiked in enumerate(model2.spike_train[-1][:nlabels]) if spiked}\n",
    "if spiked_ids:\n",
    "    print(f\"Spiked categories:\")\n",
    "    for idx, category in spiked_ids.items():\n",
    "        print(f\"\\t{idx}\\t{category}\")\n",
    "else:\n",
    "    print(\"No category spiked\")\n",
    "\n",
    "print('=' * 10)\n",
    "\n",
    "lbl_by_threshold = sorted((enumerate(model2.neuron_states[:nlabels])), key=lambda x: x[1], reverse=True)\n",
    "for i, v in lbl_by_threshold:\n",
    "    category = lbl_neurons.inverse[i][0]\n",
    "    print(f\"{i}\\t{v: 5.3f}\\t{category}\")\n",
    "print(model2.neuron_states[:nlabels])\n",
    "print(model2.neuron_thresholds[:nlabels])\n",
    "for ts in model2.spike_train:\n",
    "    print(ts[:nlabels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_threshold = 4.332\n",
    "\n",
    "\n",
    "def evaluate_paper(paper_idx):\n",
    "    # reset_model(model)\n",
    "    model2 = blank_copy(model)\n",
    "    model2.stdp = False\n",
    "    model2.neuron_thresholds[:nlabels] = [test_threshold] * nlabels\n",
    "    for t in range(2):\n",
    "        model2.add_spike(t, paper_neurons[paper_idx], strong_connection)\n",
    "    model2.simulate(1 + 2)\n",
    "    return model2.spike_train[-1][:nlabels], model2.spike_train[-1][:nlabels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdbaffb4c75f455fb9ce8d4f57d9fad1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/541 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# single-threaded (slow, but better for debugging)\n",
    "results = [evaluate_paper(paper_idx) for paper_idx in tqdm(test_idxs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tp: 490 / 524 / 541 | Perfect: 361 / 524 / 541 (correct / attempted / total)\n",
      "tp:        0.935115 | Perfect: 0.688931 | F1:        0.798696\n",
      "Precision: 0.680556 | Recall:  0.966469 | Accuracy:  0.664858\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td style=\"text-align: right;\">361</td><td style=\"text-align: right;\">490</td><td style=\"text-align: right;\">230</td><td style=\"text-align: right;\">0</td><td style=\"text-align: right;\">17</td><td style=\"text-align: right;\">524</td><td style=\"text-align: right;\">541</td><td>[0.0001, 0.0001]</td><td>[1e-05, 1e-05]</td><td style=\"text-align: right;\">2</td><td style=\"text-align: right;\">5</td><td style=\"text-align: right;\">1</td><td style=\"text-align: right;\">1e-05</td><td style=\"text-align: right;\">4.332</td><td style=\"text-align: right;\">0.2</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "'<table>\\n<tbody>\\n<tr><td style=\"text-align: right;\">361</td><td style=\"text-align: right;\">490</td><td style=\"text-align: right;\">230</td><td style=\"text-align: right;\">0</td><td style=\"text-align: right;\">17</td><td style=\"text-align: right;\">524</td><td style=\"text-align: right;\">541</td><td>[0.0001, 0.0001]</td><td>[1e-05, 1e-05]</td><td style=\"text-align: right;\">2</td><td style=\"text-align: right;\">5</td><td style=\"text-align: right;\">1</td><td style=\"text-align: right;\">1e-05</td><td style=\"text-align: right;\">4.332</td><td style=\"text-align: right;\">0.2</td></tr>\\n</tbody>\\n</table>'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tabulate import tabulate\n",
    "\n",
    "tp, fp, tn, fn = 0, 0, 0, 0\n",
    "correct = 0\n",
    "total = len(test_idxs)\n",
    "\n",
    "for actual_idx, spikes in zip(test_idxs, results):\n",
    "    correct_label = papers[actual_idx].label\n",
    "    guesses = {lbl_neurons.inverse[idx][0] for idx, spiked in enumerate(spikes[0]) if spiked}\n",
    "    guesses |= {lbl_neurons.inverse[idx][0] for idx, spiked in enumerate(spikes[1]) if spiked}\n",
    "    tp += correct_label in guesses\n",
    "    fn += not bool(guesses)  # +1 false negative if no guesses\n",
    "    fp += len([x for x in guesses if x != correct_label])\n",
    "    correct += correct_label in guesses and len(guesses) == 1\n",
    "\n",
    "n_guesses = total - fn\n",
    "print(f\"tp: {tp} / {n_guesses} / {total} | Perfect: {correct} / {n_guesses} / {total} (correct / attempted / total)\")\n",
    "print(f\"tp:        {tp / n_guesses:>.6f} | Perfect: {correct / n_guesses:>.6f} | F1:        {tp / (tp + (0.5 * (fp + fn))):>.6f}\")\n",
    "print(f\"Precision: {tp / (tp + fp):>.6f} | Recall:  {tp / (tp + fn):>.6f} | Accuracy:  {(tp + tn) / (tp + tn + fp + fn):>.6f}\")\n",
    "\n",
    "tabulate([[correct, tp, fp, tn, fn, n_guesses, total,\n",
    "           model.stdp_Apos.tolist(), model.stdp_Aneg.tolist(), lbl_threshold,\n",
    "           strong_connection, weak_connection, unknown_connection, test_threshold, test_size]], tablefmt=\"html\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sngnn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
