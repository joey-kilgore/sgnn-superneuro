{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from superneuromat.neuromorphicmodel import NeuromorphicModel\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from dataclasses import dataclass, field\n",
    "\n",
    "\n",
    "# from: https://stackoverflow.com/a/21894086/2712730\n",
    "class bidict(dict):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(bidict, self).__init__(*args, **kwargs)\n",
    "        self.inverse = {}\n",
    "        for key, value in self.items():\n",
    "            self.inverse.setdefault(value, []).append(key)\n",
    "\n",
    "    def __setitem__(self, key, value):\n",
    "        if key in self:\n",
    "            self.inverse[self[key]].remove(key)\n",
    "        super(bidict, self).__setitem__(key, value)\n",
    "        self.inverse.setdefault(value, []).append(key)\n",
    "\n",
    "    def __delitem__(self, key):\n",
    "        self.inverse.setdefault(self[key], []).remove(key)\n",
    "        if self[key] in self.inverse and not self.inverse[self[key]]:\n",
    "            del self.inverse[self[key]]\n",
    "        super(bidict, self).__delitem__(key)\n",
    "\n",
    "\n",
    "def train_test_split_indices(papers, test_size=0.2, rng=None) -> tuple[np.ndarray[int], np.ndarray[int]]:\n",
    "    \"\"\"\n",
    "    Splits a list of papers into train and test indices.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    papers : iterable of papers with ids\n",
    "        List or dict of papers to split.\n",
    "    test_size : float, optional\n",
    "        if < 1, proportion of papers to reserve for testing, by default 0.2\n",
    "        if > 1, number of papers to reserve for testing\n",
    "    rng : int, optional\n",
    "        Random state for the random number generator, default uses numpy's random\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    train_indices : list of int\n",
    "        List of indices for the training set.\n",
    "    test_indices : list of int\n",
    "        List of indices for the testing set.\n",
    "    \"\"\"\n",
    "    n = papers if isinstance(papers, int) else len(papers)\n",
    "    if test_size < 1:\n",
    "        test_size = int(np.floor(test_size * n))  # number of papers in test\n",
    "    if rng is None:\n",
    "        rng = np.random.default_rng()  # setup rng\n",
    "    elif isinstance(rng, int):\n",
    "        rng = np.random.default_rng(rng)\n",
    "\n",
    "    if isinstance(papers, int):\n",
    "        indices = np.arange(n)  # generate indices\n",
    "    elif isinstance(papers, (list, tuple)):\n",
    "        indices = papers  # assume papers is a list of indices\n",
    "    else:  # assume papers is a dict or mapping of papers with a .values() method\n",
    "        indices = [paper.idx for paper in papers.values()]  # grab indices from dict entries\n",
    "    indices = np.asarray(indices, dtype=int)\n",
    "\n",
    "    # shuffle and split\n",
    "    rng.shuffle(indices)\n",
    "    test_indices = indices[:test_size]\n",
    "    train_indices = indices[test_size:]\n",
    "    return train_indices, test_indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(2024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Case_Based',\n",
       " 'Genetic_Algorithms',\n",
       " 'Neural_Networks',\n",
       " 'Probabilistic_Methods',\n",
       " 'Reinforcement_Learning',\n",
       " 'Rule_Learning',\n",
       " 'Theory'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@dataclass\n",
    "class Paper:\n",
    "    idx: int\n",
    "    label: str\n",
    "    features: tuple[bool, ...] = ()\n",
    "    citations: list[int] = field(default_factory=list)\n",
    "\n",
    "papers = {}\n",
    "\n",
    "# Load in training data\n",
    "content = pd.read_csv(\"data/Cora/cora/cora.content\", sep=\"\\t\", header=None)\n",
    "citations = pd.read_csv(\"data/Cora/cora/cora.cites\", sep=\"\\t\", header=None)\n",
    "\n",
    "labels = set()\n",
    "\n",
    "for paper in content.itertuples(index=False):\n",
    "    idx = paper[0]\n",
    "    features = tuple([int(feature) for feature in paper[1:-1]])\n",
    "    papers[idx] = Paper(idx, paper[-1], features)\n",
    "    labels.add(paper[-1])\n",
    "\n",
    "for paper_idx, citation  in citations.itertuples(index=False):\n",
    "    papers[paper_idx].citations.append(citation)\n",
    "\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_idxs, test_idxs = train_test_split_indices(papers, test_size=0.2, rng=rng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize our model\n",
    "model = NeuromorphicModel()\n",
    "\n",
    "# Create our output neurons, set threshold very high so that we control when they spike during training.\n",
    "# dict mapping {category: neuron_id}\n",
    "lbl_neurons = bidict({label: model.create_neuron(threshold=10000) for label in labels})\n",
    "\n",
    "# Create our input neurons, one for each pixel of the image resolution.\n",
    "paper_neurons = {}  # dict mapping {paper_id: neuron_id}\n",
    "\n",
    "# make a neuron for each paper\n",
    "for paper in papers.values():\n",
    "    paper_neurons[paper.idx] = neuron_id = model.create_neuron()\n",
    "\n",
    "    if paper.idx in train_idxs:\n",
    "        # Make an explicitly STRONG synapse connecting the input to the output\n",
    "        output_id = lbl_neurons[paper.label]\n",
    "        model.create_synapse(neuron_id, output_id, weight=10001, enable_stdp=False, delay=1)\n",
    "    else:  # test set, connect this input neuron to all output neurons\n",
    "        # Connect our input neuron to output neurons\n",
    "        for output_id in lbl_neurons.values():\n",
    "            # Randomize initial weight\n",
    "            weight = (rng.uniform() * 2 - 1) * 0.00001\n",
    "            # Make a synapse connecting the input to the output\n",
    "            model.create_synapse(neuron_id, output_id, weight=weight, enable_stdp=True, delay=1)\n",
    "\n",
    "# connect papers by their citations\n",
    "for paper in papers.values():\n",
    "    for citation in paper.citations:\n",
    "        model.create_synapse(paper_neurons[paper.idx], paper_neurons[citation], weight=1.0, enable_stdp=True, delay=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop through our dataset and add spikes\n",
    "timestep = 0\n",
    "train_idxs_augmented = np.append(train_idxs, train_idxs)\n",
    "for idx in train_idxs_augmented:\n",
    "    paper = papers[idx]  # get paper by id\n",
    "    # Add a spike that will exceed the threshold for the respective label neuron\n",
    "    model.add_spike(timestep + 1, lbl_neurons[paper.label], 10001.0)\n",
    "    # Add spikes to the paper\n",
    "    model.add_spike(timestep, paper_neurons[paper.idx], 10001.0)\n",
    "    timestep += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your model has 2715 neurons and 11383 synapses.\n",
      "4334 time steps will be simulated.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4334/4334 [02:59<00:00, 24.17it/s]\n"
     ]
    }
   ],
   "source": [
    "# Set up our stdp, only one timestep because we only want it looking at the results\n",
    "# of what our input layer does to our output layer\n",
    "model.stdp_setup(time_steps=1, Apos=[0.001], Aneg=[0.000001], negative_update=True, positive_update=True)\n",
    "model.setup()\n",
    "\n",
    "print(f\"Your model has {model.num_neurons} neurons and {model.num_synapses} synapses.\\n{timestep} time steps will be simulated.\")\n",
    "\n",
    "import tqdm\n",
    "\n",
    "# Simulate\n",
    "with tqdm.tqdm(total=timestep) as pbar:\n",
    "    model.simulate(time_steps=timestep, callback=lambda _s, _t, _n: pbar.update(), use='jit')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def blank_copy(model):\n",
    "    new = NeuromorphicModel()\n",
    "    new.num_neurons = model.num_neurons\n",
    "    new.neuron_thresholds = model.neuron_thresholds.copy()\n",
    "    new.neuron_leaks = model.neuron_leaks.copy()\n",
    "    new.neuron_reset_states = model.neuron_reset_states.copy()\n",
    "    new.neuron_refractory_periods = model.neuron_refractory_periods.copy()\n",
    "    new.num_synapses = model.num_synapses\n",
    "    new.pre_synaptic_neuron_ids = model.pre_synaptic_neuron_ids.copy()\n",
    "    new.post_synaptic_neuron_ids = model.post_synaptic_neuron_ids.copy()\n",
    "    new.synaptic_weights = model.synaptic_weights.copy()\n",
    "    new.synaptic_delays = model.synaptic_delays.copy()\n",
    "    new.enable_stdp = model.enable_stdp.copy()\n",
    "    return new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 10022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed: 10023\n",
      "Paper: 1130847\tCategory: Genetic_Algorithms\n",
      "No category spiked\n",
      "==========\n",
      "1\t 4.323\tReinforcement_Learning\n",
      "3\t 4.323\tRule_Learning\n",
      "4\t 4.323\tProbabilistic_Methods\n",
      "6\t 4.323\tNeural_Networks\n",
      "5\t 4.323\tCase_Based\n",
      "2\t 4.323\tTheory\n",
      "0\t 4.323\tGenetic_Algorithms\n",
      "[4.3229814  4.3229988  4.322986   4.3229969  4.32299476 4.32298739\n",
      " 4.32299147]\n",
      "[50. 50. 50. 50. 50. 50. 50.]\n",
      "[0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "seed += 1\n",
    "print(f\"Seed: {seed}\")\n",
    "rng = np.random.default_rng(seed)\n",
    "model2 = blank_copy(model)\n",
    "model2.enable_stdp = np.zeros(model.num_neurons)\n",
    "model2.neuron_thresholds[:7] = [50] * 7\n",
    "\n",
    "# paper = random.choice(list(papers.values()))  # pick a random paper\n",
    "# paper = papers[random.choice(train_idxs)]  # pick a random paper from training set\n",
    "paper = papers[rng.choice(test_idxs)]  # pick a random paper from testing set\n",
    "\n",
    "print(f\"Paper: {paper.idx}\\tCategory: {paper.label}\")\n",
    "\n",
    "spike_n = 2\n",
    "\n",
    "for t in range(spike_n):\n",
    "    model2.add_spike(t, paper_neurons[paper.idx], 10001.)\n",
    "\n",
    "model2.setup()\n",
    "model2.simulate(spike_n + 1, use='jit')\n",
    "\n",
    "spiked_ids = {idx: lbl_neurons.inverse[idx][0]\n",
    "              for idx, spiked in enumerate(model2.spike_train[-1][:7]) if spiked}\n",
    "if spiked_ids:\n",
    "    print(f\"Spiked categories:\")\n",
    "    for idx, category in spiked_ids.items():\n",
    "        print(f\"\\t{idx}\\t{category}\")\n",
    "else:\n",
    "    print(\"No category spiked\")\n",
    "\n",
    "print('=' * 10)\n",
    "\n",
    "lbl_by_threshold = sorted((enumerate(model2._internal_states[:7])), key=lambda x: x[1], reverse=True)\n",
    "for i, v in lbl_by_threshold:\n",
    "    category = lbl_neurons.inverse[i][0]\n",
    "    print(f\"{i}\\t{v: 5.3f}\\t{category}\")\n",
    "print(model2._internal_states[:7])\n",
    "print(model2._neuron_thresholds[:7])\n",
    "for ts in model2.spike_train:\n",
    "    print(ts[:7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_paper(paper_idx):\n",
    "    model_temp = blank_copy(model)\n",
    "    model_temp.enable_stdp = np.zeros(model.num_neurons)\n",
    "    model_temp.neuron_thresholds[:7] = [50] * 7\n",
    "    for t in range(2):\n",
    "        model_temp.add_spike(t, paper_neurons[paper_idx], 10001.)\n",
    "    model_temp.setup()\n",
    "    model_temp.simulate(len(test_idxs) + 2, use='jit')\n",
    "    return model_temp.spike_train[-1][:7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = [evaluate_paper(paper_idx) for paper_idx in tqdm.tqdm(test_idxs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okay guesses: 119 / 130 / 541 | Perfect guesses: 76 / 130 / 541 (correct / attempted / total)\n",
      "Okay guesses: 0.915384615 | Perfect guesses: 0.584615385\n",
      "Precision:    0.224528302 | Accuracy:        0.126461211\n"
     ]
    }
   ],
   "source": [
    "tp = 0\n",
    "correct = 0\n",
    "total = len(test_idxs)\n",
    "fp = total\n",
    "tn = 0\n",
    "fn = 0\n",
    "\n",
    "for actual_idx, spikes in zip(test_idxs, results):\n",
    "    correct_label = papers[actual_idx].label\n",
    "    guesses = [lbl_neurons.inverse[idx][0] for idx, spiked in enumerate(spikes) if spiked]\n",
    "    if correct_label in guesses:\n",
    "        tp += 1\n",
    "        fp -= 1\n",
    "        if len(guesses) == 1:\n",
    "            correct += 1\n",
    "    elif guesses:\n",
    "        fp -= 1\n",
    "    else:\n",
    "        fn += 1\n",
    "\n",
    "n_guesses = total - fp\n",
    "print(f\"Okay guesses: {tp} / {n_guesses} / {total} | Perfect guesses: {correct} / {n_guesses} / {total} (correct / attempted / total)\")\n",
    "print(f\"Okay guesses: {tp / n_guesses:>.9f} | Perfect guesses: {correct / n_guesses:>.9f}\")\n",
    "print(f\"Precision:    {tp / (tp + fp):>.9f} | Accuracy:        {(tp + tn) / (tp + tn + fp + fn):>.9f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sngnn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
