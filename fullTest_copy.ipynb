{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import argparse\n",
    "from multiprocessing import Pool\n",
    "from superneuromat.neuromorphicmodel import NeuromorphicModel\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphData():\n",
    "    def __init__(self, name, config):\n",
    "        self.name = name\n",
    "        self.config = config\n",
    "        self.paper_to_topic = {} # maps the paper ID in the dataset to its topic ID\n",
    "        self.index_to_paper = []    # creates an index for each paper\n",
    "        self.topics = []            # the list of topics \n",
    "        self.train_papers = []\n",
    "        self.validation_papers = []\n",
    "        self.test_papers = []\n",
    "        self.load_topics()\n",
    "        self.train_val_test_split()\n",
    "        self.load_features()\n",
    "        self.load_graph()\n",
    "\n",
    "    def load_topics(self):\n",
    "        if (self.name == \"cora\"):\n",
    "            f = open(\"data/Cora/group-edges.csv\", 'r')\n",
    "            lines = f.readlines()\n",
    "            f.close()\n",
    "\n",
    "            for line in lines:\n",
    "                fields = line.strip().split(\",\")\n",
    "                if (fields[1] not in self.topics):\n",
    "                    self.topics.append(fields[1])\n",
    "                self.paper_to_topic[fields[0]] = fields[1]\n",
    "                self.index_to_paper.append(fields[0])\n",
    "        elif (self.name == \"citeseer\"):\n",
    "            f = open(\"data/citeseer/citeseer.content\", 'r')\n",
    "            lines = f.readlines()\n",
    "            f.close()\n",
    "\n",
    "            for line in lines:\n",
    "                fields = line.strip().split()\n",
    "                if (fields[-1] not in self.topics):\n",
    "                    self.topics.append(fields[-1])\n",
    "                print(fields[0])\n",
    "                self.paper_to_topic[fields[0]] = fields[-1]\n",
    "                self.index_to_paper.append(fields[0])   \n",
    "        elif (self.name == \"pubmed\"):\n",
    "            f = open(\"data/Pubmed-Diabetes/data/Pubmed-Diabetes.NODE.paper.tab\", 'r')\n",
    "            lines = f.readlines()\n",
    "            f.close()\n",
    "            lines = lines[2:]\n",
    "        \n",
    "            for line in lines:\n",
    "                fields = line.strip().split()\n",
    "                if (fields[1] not in self.topics):\n",
    "                    self.topics.append(fields[1])\n",
    "                self.paper_to_topic[\"paper:\"+fields[0]] = fields[1]\n",
    "                self.index_to_paper.append(\"paper:\"+fields[0])\n",
    "\n",
    "    def load_features(self):\n",
    "        self.features = {} # keyed on paper ID, value is the feature vector\n",
    "        if (self.name == \"cora\"):\n",
    "            f = open(\"data/Cora/cora/cora.content\", 'r')\n",
    "            lines = f.readlines()\n",
    "            f.close()\n",
    "            \n",
    "            for line in lines:\n",
    "                fields = line.strip().split()\n",
    "                paper_id = fields[0]\n",
    "                feature = [int(x) for x in fields[1:-1]]\n",
    "                self.features[paper_id] = feature\n",
    "                self.num_features = len(feature)\n",
    "        elif (self.name == \"citeseer\"):\n",
    "            f = open(\"data/citeseer/citeseer.content\", 'r')\n",
    "            lines = f.readlines()\n",
    "            f.close()\n",
    "            for line in lines:\n",
    "                fields = line.strip().split()\n",
    "                paper_id = fields[0]\n",
    "                feature = [int(x) for x in fields[1:-1]]\n",
    "                self.features[paper_id] = feature\n",
    "                self.num_features = len(feature)\n",
    "            print(\"NUM PAPERS WITH FEATURES: \", len(self.features.keys()))\n",
    "        elif (self.name == \"pubmed\"):\n",
    "            f = open(\"data/Pubmed-Diabetes/data/Pubmed-Diabetes.NODE.paper.tab\", 'r')\n",
    "            lines = f.readlines()\n",
    "            f.close()\n",
    "            feature_line = lines[1]\n",
    "            fields = feature_line.split()\n",
    "            fields = fields[1:-1]\n",
    "            all_features = {}\n",
    "            for i in range(len(fields)):\n",
    "                feat = fields[i].split(':')[1]\n",
    "                all_features[feat] = i\n",
    "\n",
    "            self.num_features = len(all_features.keys())\n",
    "            lines = lines[2:]\n",
    "            for line in lines:\n",
    "                feature = [0]*self.num_features\n",
    "                fields = line.split()\n",
    "                paper_id = \"paper:\" + fields[0]\n",
    "                xfields = fields[-1].split('=')\n",
    "                xfields = xfields[1].split(',')\n",
    "                for x in xfields:\n",
    "                    feature[all_features[x]] = 1\n",
    "                self.features[paper_id] = feature\n",
    "\n",
    "        self.paper_to_features = {} # keyed on paper ID, value is the number of features it has\n",
    "        self.feature_to_papers = {} # keyed on feature ID, value is the number of papers that have that feature\n",
    "        for p in self.features.keys():\n",
    "            self.paper_to_features[p] = np.sum(self.features[p])\n",
    "            for i in range(len(self.features[p])):\n",
    "                if (i not in self.feature_to_papers.keys()):\n",
    "                    self.feature_to_papers[i] = 0\n",
    "                self.feature_to_papers[i] += self.features[p][i]\n",
    " \n",
    "    def load_graph(self):\n",
    "        if (self.name == \"cora\"):\n",
    "            self.graph = nx.read_edgelist(\"data/Cora/edges.csv\", delimiter=\",\")\n",
    "\n",
    "        elif (self.name == \"citeseer\"):\n",
    "            self.graph = nx.read_edgelist(\"data/citeseer/citeseer.cites\")\n",
    "\n",
    "        elif (self.name == \"pubmed\"):\n",
    "            self.graph = nx.read_edgelist(\"data/Pubmed-Diabetes/data/edge_list.csv\", delimiter=\",\")\n",
    "\n",
    "    def train_val_test_split(self):\n",
    "        np.random.seed(self.config[\"seed\"])\n",
    "        train_papers = []\n",
    "        test_papers = []\n",
    "        validation_papers = []\n",
    "        check_breakdown = {}\n",
    "        \n",
    "        for k in self.topics:\n",
    "            check_breakdown[k] = 0\n",
    "        \n",
    "        while (len(train_papers) < len(self.topics)*20):\n",
    "            index = np.random.randint(len(self.index_to_paper))\n",
    "            topic = self.paper_to_topic[self.index_to_paper[index]]\n",
    "            if (check_breakdown[topic] < 20 and self.index_to_paper[index] not in train_papers):\n",
    "                train_papers.append(self.index_to_paper[index])\n",
    "                check_breakdown[topic] += 1\n",
    "        \n",
    "        while (len(validation_papers) < len(self.topics)*20):\n",
    "            index = np.random.randint(len(self.index_to_paper))\n",
    "            topic = self.paper_to_topic[self.index_to_paper[index]]\n",
    "            if (check_breakdown[topic] < 40 and self.index_to_paper[index] not in train_papers and self.index_to_paper[index] not in validation_papers):\n",
    "                validation_papers.append(self.index_to_paper[index])\n",
    "                check_breakdown[topic] += 1\n",
    "        \n",
    "        for i in range(len(self.index_to_paper)):\n",
    "            if (self.index_to_paper[i] not in train_papers and self.index_to_paper[i] not in validation_papers and self.index_to_paper[i] in self.paper_to_topic.keys()):\n",
    "                test_papers.append(self.index_to_paper[i])\n",
    "\n",
    "        self.train_papers = train_papers\n",
    "        self.test_papers = test_papers\n",
    "        self.validation_papers = validation_papers \n",
    "\n",
    "def print_paper_spikes(model, paper_neurons):\n",
    "    \"\"\"\n",
    "    Print which paper neurons spiked at each time step, \n",
    "    assuming model.spike_train[t] is a binary array \n",
    "    indicating spike (1) or no spike (0) for each neuron.\n",
    "    \"\"\"\n",
    "    for t, spike_array in enumerate(model.spike_train):\n",
    "        # 'spike_array' is presumably something like [0, 1, 0, 0, 1, ...]\n",
    "        spiking_papers = []\n",
    "        for paper_id, neuron_id in paper_neurons.items():\n",
    "            # Check if the value at index 'neuron_id' is 1 (i.e., a spike)\n",
    "            if spike_array[neuron_id] == 1:\n",
    "                spiking_papers.append(neuron_id)\n",
    "\n",
    "        if spiking_papers:\n",
    "            print(f\"Time {t}: Paper Neurons that spiked: {spiking_papers}\")\n",
    "\n",
    "\n",
    "def load_network(graph, config):\n",
    "    from superneuromat.neuromorphicmodel import NeuromorphicModel\n",
    "    import numpy as np\n",
    "\n",
    "    # Initialize the Neuromorphic Model\n",
    "    model = NeuromorphicModel()\n",
    "\n",
    "    # Dictionaries to hold neurons\n",
    "    topic_neurons = {}\n",
    "    paper_neurons = {}\n",
    "    feature_neurons = {}\n",
    "\n",
    "    # Create topic neurons\n",
    "    for t in graph.topics:\n",
    "        neuron = model.create_neuron(threshold=config[\"topic_threshold\"])\n",
    "        topic_neurons[t] = neuron\n",
    "        # Assuming additional parameters can be set directly\n",
    "        neuron.tau_m = config[\"topic_leak\"]\n",
    "        neuron.V_m = 0\n",
    "        neuron.V_reset = 0\n",
    "        neuron.E_L = 0\n",
    "        neuron.tau_minus = config[\"topic_tau_minus\"]\n",
    "\n",
    "    # Create paper neurons\n",
    "    for node in graph.graph.nodes:\n",
    "        if node not in graph.paper_to_topic:\n",
    "            continue\n",
    "        neuron = model.create_neuron(threshold=config[\"paper_threshold\"])\n",
    "        paper_neurons[node] = neuron\n",
    "        neuron.tau_m = config[\"paper_leak\"]\n",
    "        neuron.V_m = 0\n",
    "        neuron.V_reset = 0\n",
    "        neuron.E_L = 0\n",
    "        neuron.tau_minus = config[\"paper_tau_minus\"]\n",
    "\n",
    "        # Set refractory periods based on paper type\n",
    "        if node in graph.train_papers:\n",
    "            neuron.t_ref = config[\"train_ref\"]\n",
    "        elif node in graph.validation_papers:\n",
    "            neuron.t_ref = config[\"validation_ref\"]\n",
    "        elif node in graph.test_papers:\n",
    "            neuron.t_ref = config[\"test_ref\"]\n",
    "\n",
    "    # Create feature neurons if features are enabled\n",
    "    if config[\"features\"] == 1:\n",
    "        for i in range(graph.num_features):\n",
    "            neuron = model.create_neuron(threshold=config[\"feature_threshold\"])\n",
    "            feature_neurons[i] = neuron\n",
    "            neuron.tau_m = config[\"feature_leak\"]\n",
    "            neuron.V_m = 0\n",
    "            neuron.V_reset = 0\n",
    "            neuron.E_L = 0\n",
    "            neuron.tau_minus = config[\"feature_tau_minus\"]\n",
    "            neuron.t_ref = config[\"feature_ref\"]\n",
    "\n",
    "    # Connect paper neurons based on the graph edges\n",
    "    for edge in graph.graph.edges:\n",
    "        if edge[0] not in paper_neurons or edge[1] not in paper_neurons:\n",
    "            continue\n",
    "        pre = paper_neurons[edge[0]]\n",
    "        post = paper_neurons[edge[1]]\n",
    "        variation_scale = 0.01\n",
    "\n",
    "        w = config[\"graph_weight\"]\n",
    "        w *= (1.0 + np.random.normal(0, variation_scale))\n",
    "        d = int(config[\"graph_delay\"])\n",
    "        model.create_synapse(pre, post, weight=w, delay=d, enable_stdp=False)\n",
    "        model.create_synapse(post, pre, weight=w, delay=d, enable_stdp=False)\n",
    "\n",
    "    # Connect training papers to topics\n",
    "    for paper in graph.train_papers:\n",
    "        paper_neuron = paper_neurons[paper]\n",
    "        topic_neuron = topic_neurons[graph.paper_to_topic[paper]]\n",
    "        variation_scale = 0.01\n",
    "\n",
    "        w = config[\"train_to_topic_weight\"]\n",
    "        w *= (1.0 + np.random.normal(0, variation_scale))\n",
    "        d = int(config[\"train_to_topic_delay\"])\n",
    "        model.create_synapse(paper_neuron, topic_neuron, weight=w, delay=d, enable_stdp=False)\n",
    "        model.create_synapse(topic_neuron, paper_neuron, weight=w, delay=d, enable_stdp=False)\n",
    "\n",
    "    # Connect validation papers to topics with STDP\n",
    "    for paper in graph.validation_papers:\n",
    "        for topic in graph.topics:\n",
    "            paper_neuron = paper_neurons[paper]\n",
    "            topic_neuron = topic_neurons[topic]\n",
    "            variation_scale = 0.01\n",
    "\n",
    "            w = config[\"validation_to_topic_weight\"]\n",
    "            w *= (1.0 + np.random.normal(0, variation_scale))\n",
    "            d = int(config[\"validation_to_topic_delay\"])\n",
    "            model.create_synapse(\n",
    "                paper_neuron, topic_neuron,\n",
    "                weight=w, delay=d, enable_stdp=True\n",
    "            )\n",
    "            model.create_synapse(\n",
    "                topic_neuron, paper_neuron,\n",
    "                weight=w, delay=d, enable_stdp=True\n",
    "            )\n",
    "\n",
    "    # Connect test papers to topics with STDP\n",
    "    for paper in graph.test_papers:\n",
    "        for topic in graph.topics:\n",
    "            paper_neuron = paper_neurons[paper]\n",
    "            topic_neuron = topic_neurons[topic]\n",
    "            variation_scale = 0.01 \n",
    "\n",
    "            w = config[\"test_to_topic_weight\"]\n",
    "            w *= (1.0 + np.random.normal(0, variation_scale))\n",
    "            d = int(config[\"test_to_topic_delay\"])\n",
    "            model.create_synapse(\n",
    "                paper_neuron, topic_neuron,\n",
    "                weight=w, delay=d, enable_stdp=True\n",
    "            )\n",
    "            model.create_synapse(\n",
    "                topic_neuron, paper_neuron,\n",
    "                weight=w, delay=d, enable_stdp=True\n",
    "            )\n",
    "\n",
    "    # Connect features to paper neurons if features are enabled\n",
    "    if config[\"features\"] == 1:\n",
    "        for node in graph.graph.nodes:\n",
    "            if node not in graph.features:\n",
    "                print(\"No features for node: \", node)\n",
    "                continue\n",
    "            for i in range(len(graph.features[node])):\n",
    "                if graph.features[node][i] == 1:\n",
    "                    paper_neuron = paper_neurons[node]\n",
    "                    feature_neuron = feature_neurons[i]\n",
    "                    variation_scale = 0.01\n",
    "\n",
    "                    w = config[\"paper_to_feature_weight\"]\n",
    "                    w *= (1.0 + np.random.normal(0, variation_scale))\n",
    "                    d = int(config[\"paper_to_feature_delay\"])\n",
    "                    stdp_on = config[\"paper_to_feature_stdp\"]\n",
    "                    model.create_synapse(\n",
    "                        paper_neuron, feature_neuron,\n",
    "                        weight=w, delay=d, enable_stdp=stdp_on\n",
    "                    )\n",
    "                    model.create_synapse(\n",
    "                        feature_neuron, paper_neuron,\n",
    "                        weight=w, delay=d, enable_stdp=stdp_on\n",
    "                    )\n",
    "\n",
    "    return model, paper_neurons, topic_neurons, feature_neurons\n",
    "\n",
    "def load_network(graph, config):\n",
    "\n",
    "    # Initialize the Neuromorphic Model\n",
    "    model = NeuromorphicModel()\n",
    "\n",
    "    # Dictionaries to hold neuron IDs\n",
    "    topic_neurons = {}\n",
    "    paper_neurons = {}\n",
    "    feature_neurons = {}\n",
    "\n",
    "    # Create paper neurons\n",
    "    for node in graph.graph.nodes:\n",
    "        if (node not in graph.paper_to_topic.keys()):\n",
    "            continue\n",
    "        neuron_id = model.create_neuron(\n",
    "            threshold=config[\"paper_threshold\"],\n",
    "            leak=config[\"paper_leak\"],\n",
    "            reset_state=0.0,\n",
    "            refractory_period=0,  # Will set below based on paper type\n",
    "        )\n",
    "        paper_neurons[node] = neuron_id\n",
    "\n",
    "        # Set refractory periods based on paper type\n",
    "        if node in graph.train_papers:\n",
    "            model.neuron_refractory_periods[neuron_id] = config[\"train_ref\"]\n",
    "        elif node in graph.validation_papers:\n",
    "            model.neuron_refractory_periods[neuron_id] = config[\"validation_ref\"]\n",
    "        elif node in graph.test_papers:\n",
    "            model.neuron_refractory_periods[neuron_id] = config[\"test_ref\"]\n",
    "\n",
    "    # Create topic neurons\n",
    "    for t in graph.topics:\n",
    "        neuron_id = model.create_neuron(\n",
    "            threshold=config[\"topic_threshold\"],\n",
    "            leak=config[\"topic_leak\"],\n",
    "            reset_state=0.0,\n",
    "            refractory_period=0, \n",
    "        )\n",
    "        topic_neurons[t] = neuron_id\n",
    "        # Setting tau_minus is not directly supported\n",
    "\n",
    "    # Create feature neurons if features are enabled\n",
    "    if config[\"features\"] == 1:\n",
    "        for i in range(graph.num_features):\n",
    "            neuron_id = model.create_neuron(\n",
    "                threshold=config[\"feature_threshold\"],\n",
    "                leak=config[\"feature_leak\"],\n",
    "                reset_state=0.0,\n",
    "                refractory_period=config[\"feature_ref\"],\n",
    "            )\n",
    "            feature_neurons[i] = neuron_id\n",
    "\n",
    "    # Connect paper neurons based on the graph edges\n",
    "    for edge in graph.graph.edges:\n",
    "        if (edge[0] not in graph.paper_to_topic.keys() or edge[1] not in graph.paper_to_topic.keys()):\n",
    "            continue\n",
    "        pre_id = paper_neurons[edge[0]]\n",
    "        post_id = paper_neurons[edge[1]]\n",
    "        variation_scale = 0.01\n",
    "\n",
    "        w = config[\"graph_weight\"]\n",
    "        w *= (1.0 + np.random.normal(0, variation_scale))\n",
    "        d = int(config[\"graph_delay\"])\n",
    "        model.create_synapse(pre_id, post_id, weight=w, delay=d, enable_stdp=False)\n",
    "        model.create_synapse(post_id, pre_id, weight=w, delay=d, enable_stdp=False)\n",
    "\n",
    "    # Connect training papers to topics\n",
    "    for paper in graph.train_papers:\n",
    "        paper_id = paper_neurons[paper]\n",
    "        topic_id = topic_neurons[graph.paper_to_topic[paper]]\n",
    "        variation_scale = 0.01 \n",
    "\n",
    "        w = config[\"train_to_topic_weight\"]\n",
    "        w *= (1.0 + np.random.normal(0, variation_scale))\n",
    "        d = int(config[\"train_to_topic_delay\"])\n",
    "        model.create_synapse(paper_id, topic_id, weight=w, delay=d, enable_stdp=False)\n",
    "        model.create_synapse(topic_id, paper_id, weight=w, delay=d, enable_stdp=False)\n",
    "\n",
    "    # Connect validation papers to topics with STDP\n",
    "    for paper in graph.validation_papers:\n",
    "        paper_id = paper_neurons[paper]\n",
    "        for topic in graph.topics:\n",
    "            topic_id = topic_neurons[topic]\n",
    "            variation_scale = 0.01 \n",
    "\n",
    "            w = config[\"validation_to_topic_weight\"]\n",
    "            w *= (1.0 + np.random.normal(0, variation_scale))\n",
    "            d = int(config[\"validation_to_topic_delay\"])\n",
    "            stdp_on = True\n",
    "            model.create_synapse(\n",
    "                paper_id, topic_id, weight=w, delay=d, enable_stdp=stdp_on\n",
    "            )\n",
    "            model.create_synapse(\n",
    "                topic_id, paper_id, weight=w, delay=d, enable_stdp=stdp_on\n",
    "            )\n",
    "\n",
    "    # Connect test papers to topics with STDP\n",
    "    for paper in graph.test_papers:\n",
    "        paper_id = paper_neurons[paper]\n",
    "        for topic in graph.topics:\n",
    "            topic_id = topic_neurons[topic]\n",
    "            variation_scale = 0.01 \n",
    "\n",
    "            w = config[\"test_to_topic_weight\"]\n",
    "            w *= (1.0 + np.random.normal(0, variation_scale))\n",
    "            d = int(config[\"test_to_topic_delay\"])\n",
    "            stdp_on = True\n",
    "            model.create_synapse(\n",
    "                paper_id, topic_id, weight=w, delay=d, enable_stdp=stdp_on\n",
    "            )\n",
    "            model.create_synapse(\n",
    "                topic_id, paper_id, weight=w, delay=d, enable_stdp=stdp_on\n",
    "            )\n",
    "\n",
    "    # Connect features to paper neurons if features are enabled\n",
    "    if config[\"features\"] == 1:\n",
    "        for node in graph.graph.nodes:\n",
    "            if (node not in graph.features.keys()):\n",
    "                print(\"No features for node: \", node)\n",
    "                continue\n",
    "            paper_id = paper_neurons[node]\n",
    "            for i, feature_value in enumerate(graph.features[node]):\n",
    "                if feature_value == 1:\n",
    "                    feature_id = feature_neurons[i]\n",
    "                    variation_scale = 0.01\n",
    "\n",
    "                    w = config[\"paper_to_feature_weight\"]\n",
    "                    w *= (1.0 + np.random.normal(0, variation_scale))\n",
    "                    d = int(config[\"paper_to_feature_delay\"])\n",
    "                    stdp_on = config[\"paper_to_feature_stdp\"]\n",
    "                    # Adjust weights if weighted connections are enabled\n",
    "                    if config.get(\"paper_to_feature_weighted\", False):\n",
    "                        w_paper_to_feature = (\n",
    "                            config[\"paper_to_feature_weight\"]\n",
    "                            * graph.paper_to_features[node]\n",
    "                            / len(graph.features[node])\n",
    "                        )\n",
    "                        w_feature_to_paper = (\n",
    "                            config[\"feature_to_paper_weight\"]\n",
    "                            * graph.feature_to_papers[i]\n",
    "                            / len(graph.features)\n",
    "                        )\n",
    "                    else:\n",
    "                        w_paper_to_feature = w\n",
    "                        w_feature_to_paper = w\n",
    "\n",
    "                    # Create synapses\n",
    "                    model.create_synapse(\n",
    "                        paper_id,\n",
    "                        feature_id,\n",
    "                        weight=w_paper_to_feature,\n",
    "                        delay=d,\n",
    "                        enable_stdp=stdp_on,\n",
    "                    )\n",
    "                    model.create_synapse(\n",
    "                        feature_id,\n",
    "                        paper_id,\n",
    "                        weight=w_feature_to_paper,\n",
    "                        delay=d,\n",
    "                        enable_stdp=stdp_on,\n",
    "                    )\n",
    "\n",
    "    return model, paper_neurons, topic_neurons, feature_neurons\n",
    "\n",
    "def test_paper(x):\n",
    "    paper = x[0]    # paper ID of the paper you want to test\n",
    "    graph = x[1]    # loaded graph\n",
    "    config = x[2]   # config parameters\n",
    "\n",
    "    # Load the network\n",
    "    model, paper_neurons, topic_neurons, feature_neurons = load_network(graph, config)\n",
    "\n",
    "    # Add a spike to the test paper neuron at time step 1 with a value sufficient to trigger a spike\n",
    "    test_paper_id = paper_neurons[paper]\n",
    "    model.add_spike(0, test_paper_id, value=config.get(\"input_spike_value\", 100.0))\n",
    "\n",
    "    correct_topic_label = graph.paper_to_topic[paper]  # e.g. \"Neural_Nets\"\n",
    "    correct_topic_id = topic_neurons[correct_topic_label]\n",
    "    #model.add_spike(1, correct_topic_id, value=100.0)\n",
    "\n",
    "    # Determine the number of time steps for STDP\n",
    "    simtime = int(config.get(\"simtime\", 20)) \n",
    "    time_steps = int(config.get(\"stdp_time_steps\", simtime))\n",
    "\n",
    "    # Set up STDP parameters if not already set\n",
    "    if not model.stdp:\n",
    "        apos_value = config.get(\"stdp_Apos\", 0.1)\n",
    "        aneg_value = config.get(\"stdp_Aneg\", 0.0001)\n",
    "\n",
    "        # Create lists of Apos and Aneg values with length equal to time_steps\n",
    "        Apos = [apos_value] * time_steps\n",
    "        Aneg = [aneg_value] * time_steps\n",
    "        model.stdp_setup(\n",
    "            time_steps=time_steps,\n",
    "            Apos=Apos,\n",
    "            Aneg=Aneg,\n",
    "            positive_update=True,\n",
    "            negative_update=True,\n",
    "        )\n",
    "\n",
    "    # Prepare the model for simulation\n",
    "    model.setup()\n",
    "\n",
    "    # Simulate the model\n",
    "    model.simulate(time_steps=simtime)\n",
    "    #print(\"Printing\", test_paper_id, correct_topic_id)\n",
    "    #print_paper_spikes(model, topic_neurons)\n",
    "\n",
    "    \n",
    "    # Analyze the weights between the test paper neuron and topic neurons\n",
    "    #min_weight = float('inf')\n",
    "    min_weight = -100\n",
    "    min_topic = None\n",
    "    for topic, topic_id in topic_neurons.items():\n",
    "        # Find the synapse from topic neuron to test paper neuron\n",
    "        synapse_indices = [\n",
    "            i for i, (pre, post) in enumerate(zip(model.pre_synaptic_neuron_ids, model.post_synaptic_neuron_ids))\n",
    "            #if pre == topic_id and post == test_paper_id\n",
    "            if pre == test_paper_id and post == topic_id\n",
    "        ]\n",
    "        if synapse_indices:\n",
    "            idx = synapse_indices[0]\n",
    "            weight = model.synaptic_weights[idx]\n",
    "            print(f\"Topic: {topic}, Paper: {paper}, Weight: {weight}\")\n",
    "            if weight > min_weight:\n",
    "                min_weight = weight\n",
    "                min_topic = topic\n",
    "\n",
    "    # Determine if the predicted topic matches the actual topic\n",
    "    actual_topic = graph.paper_to_topic[paper]\n",
    "    retval = 1 if actual_topic == min_topic else 0\n",
    "    if retval == 1:\n",
    "        print(f\"MIN VAL for {paper} Topic {min_topic} CORRECT\")\n",
    "    else:\n",
    "        print(f\"MIN VAL for {paper} Topic {min_topic} WRONG, Expected {actual_topic}\")\n",
    "\n",
    "    # Optionally, plot the weights\n",
    "    if config.get(\"monitors\", False):\n",
    "        weights = []\n",
    "        topics = []\n",
    "        for topic, topic_id in topic_neurons.items():\n",
    "            synapse_indices = [\n",
    "                i for i, (pre, post) in enumerate(zip(model.pre_synaptic_neuron_ids, model.post_synaptic_neuron_ids))\n",
    "                if pre == topic_id and post == test_paper_id\n",
    "            ]\n",
    "            if synapse_indices:\n",
    "                idx = synapse_indices[0]\n",
    "                weights.append(model.synaptic_weights[idx])\n",
    "                topics.append(topic)\n",
    "        plt.bar(topics, weights)\n",
    "        plt.xlabel('Topics')\n",
    "        plt.ylabel('Weights')\n",
    "        plt.title('Weights from Topic Neurons to Test Paper Neuron')\n",
    "        plt.show()\n",
    "\n",
    "    return retval\n",
    "\n",
    "def evaluate_network(graph, test_papers, get_predicted_topic_for_paper):\n",
    "    true_labels = []\n",
    "    pred_labels = []\n",
    "\n",
    "    for paper in test_papers:\n",
    "        true_label = graph.paper_to_topic[paper]\n",
    "        predicted_label = get_predicted_topic_for_paper(paper)  # your code here\n",
    "        true_labels.append(true_label)\n",
    "        pred_labels.append(predicted_label)\n",
    "\n",
    "    # Unique topics used\n",
    "    topics_list = list(set(true_labels + pred_labels))\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(true_labels, pred_labels, labels=topics_list)\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(cm)\n",
    "\n",
    "    # Classification report\n",
    "    report = classification_report(true_labels, pred_labels, labels=topics_list, target_names=topics_list)\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(report)\n",
    "\n",
    "    # Optional: plot confusion matrix\n",
    "    plot_confusion_matrix(cm, topics_list)\n",
    "\n",
    "def plot_confusion_matrix(cm, labels):\n",
    "    fig, ax = plt.subplots()\n",
    "    cax = ax.matshow(cm, cmap=plt.cm.Blues)\n",
    "    fig.colorbar(cax)\n",
    "    ax.set_xticks(np.arange(len(labels)))\n",
    "    ax.set_yticks(np.arange(len(labels)))\n",
    "    ax.set_xticklabels(labels, rotation=90)\n",
    "    ax.set_yticklabels(labels)\n",
    "\n",
    "    for i in range(len(labels)):\n",
    "        for j in range(len(labels)):\n",
    "            ax.text(j, i, cm[i, j],\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"black\")\n",
    "\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"True\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: Rule_Learning, Paper: 187260, Weight: 7.345330628014758e-06\n",
      "Topic: Neural_Networks, Paper: 187260, Weight: 4.483751368208568e-06\n",
      "Topic: Case_Based, Paper: 187260, Weight: 8.047279008618739e-07\n",
      "Topic: Genetic_Algorithms, Paper: 187260, Weight: 9.428424744773145e-06\n",
      "Topic: Theory, Paper: 187260, Weight: 1.346633686765463e-05\n",
      "Topic: Reinforcement_Learning, Paper: 187260, Weight: -6.358868716861151e-06\n",
      "Topic: Probabilistic_Methods, Paper: 187260, Weight: -1.0591925217924038e-06\n",
      "MIN VAL for 187260 Topic Theory WRONG, Expected Case_Based\n",
      "Topic: Rule_Learning, Paper: 1118658, Weight: -9.571848618946023e-06\n",
      "Topic: Neural_Networks, Paper: 1118658, Weight: -2.5483699564355663e-05\n",
      "Topic: Case_Based, Paper: 1118658, Weight: 2.1862697096441892e-05\n",
      "Topic: Genetic_Algorithms, Paper: 1118658, Weight: -1.5974693397060804e-05\n",
      "Topic: Theory, Paper: 1118658, Weight: 6.046720929713526e-06\n",
      "Topic: Reinforcement_Learning, Paper: 1118658, Weight: 9.898099658897202e-06\n",
      "Topic: Probabilistic_Methods, Paper: 1118658, Weight: 1.2246580936120462e-05\n",
      "MIN VAL for 1118658 Topic Case_Based WRONG, Expected Theory\n",
      "Topic: Rule_Learning, Paper: 1127863, Weight: 2.570562641259264e-06\n",
      "Topic: Neural_Networks, Paper: 1127863, Weight: -2.6690909522490258e-06\n",
      "Topic: Case_Based, Paper: 1127863, Weight: -2.268949944437686e-06\n",
      "Topic: Genetic_Algorithms, Paper: 1127863, Weight: -8.203300340845946e-06\n",
      "Topic: Theory, Paper: 1127863, Weight: 4.697002245621589e-06\n",
      "Topic: Reinforcement_Learning, Paper: 1127863, Weight: -5.845605726272047e-06\n",
      "Topic: Probabilistic_Methods, Paper: 1127863, Weight: 2.802254351846786e-06\n",
      "MIN VAL for 1127863 Topic Theory WRONG, Expected Rule_Learning\n",
      "Topic: Rule_Learning, Paper: 237376, Weight: 6.307145183371746e-06\n",
      "Topic: Neural_Networks, Paper: 237376, Weight: -9.80841233960865e-06\n",
      "Topic: Case_Based, Paper: 237376, Weight: -1.952189379558216e-05\n",
      "Topic: Genetic_Algorithms, Paper: 237376, Weight: 6.908754421028844e-06\n",
      "Topic: Theory, Paper: 237376, Weight: 1.215563517887374e-05\n",
      "Topic: Reinforcement_Learning, Paper: 237376, Weight: -7.179419201782587e-06\n",
      "Topic: Probabilistic_Methods, Paper: 237376, Weight: -1.3251824889605257e-05\n",
      "MIN VAL for 237376 Topic Theory CORRECT\n",
      "Topic: Rule_Learning, Paper: 4649, Weight: -1.1012636369520768e-05\n",
      "Topic: Neural_Networks, Paper: 4649, Weight: 1.1664333943613446e-05\n",
      "Topic: Case_Based, Paper: 4649, Weight: -9.855584805895976e-06\n",
      "Topic: Genetic_Algorithms, Paper: 4649, Weight: 7.742455654685053e-06\n",
      "Topic: Theory, Paper: 4649, Weight: -4.753693402027528e-06\n",
      "Topic: Reinforcement_Learning, Paper: 4649, Weight: 1.6045079187961892e-05\n",
      "Topic: Probabilistic_Methods, Paper: 4649, Weight: 5.209612577102945e-06\n",
      "MIN VAL for 4649 Topic Reinforcement_Learning WRONG, Expected Rule_Learning\n",
      "Topic: Rule_Learning, Paper: 1136634, Weight: 2.795908285227103e-06\n",
      "Topic: Neural_Networks, Paper: 1136634, Weight: -1.0287263157226551e-05\n",
      "Topic: Case_Based, Paper: 1136634, Weight: 9.260501668400292e-06\n",
      "Topic: Genetic_Algorithms, Paper: 1136634, Weight: 4.2014416450462e-06\n",
      "Topic: Theory, Paper: 1136634, Weight: 1.1096826246964256e-05\n",
      "Topic: Reinforcement_Learning, Paper: 1136634, Weight: -5.090426269871499e-06\n",
      "Topic: Probabilistic_Methods, Paper: 1136634, Weight: -8.288326672820453e-06\n",
      "MIN VAL for 1136634 Topic Theory CORRECT\n",
      "Topic: Rule_Learning, Paper: 91852, Weight: -7.604624612741741e-06\n",
      "Topic: Neural_Networks, Paper: 91852, Weight: 7.08252993602201e-06\n",
      "Topic: Case_Based, Paper: 91852, Weight: -6.097668541286803e-06\n",
      "Topic: Genetic_Algorithms, Paper: 91852, Weight: -8.743614549857232e-06\n",
      "Topic: Theory, Paper: 91852, Weight: 1.2442332190244835e-05\n",
      "Topic: Reinforcement_Learning, Paper: 91852, Weight: 8.645834463978462e-06\n",
      "Topic: Probabilistic_Methods, Paper: 91852, Weight: 1.9072386451488021e-06\n",
      "MIN VAL for 91852 Topic Theory WRONG, Expected Case_Based\n",
      "Topic: Rule_Learning, Paper: 52847, Weight: -9.861279315526525e-06\n",
      "Topic: Neural_Networks, Paper: 52847, Weight: 5.807023244195475e-06\n",
      "Topic: Case_Based, Paper: 52847, Weight: -9.648587175062573e-06\n",
      "Topic: Genetic_Algorithms, Paper: 52847, Weight: -1.1026672346465936e-05\n",
      "Topic: Theory, Paper: 52847, Weight: -5.427144914531425e-06\n",
      "Topic: Reinforcement_Learning, Paper: 52847, Weight: 4.428855449301381e-06\n",
      "Topic: Probabilistic_Methods, Paper: 52847, Weight: 1.377475915023037e-05\n",
      "MIN VAL for 52847 Topic Probabilistic_Methods WRONG, Expected Theory\n",
      "Topic: Rule_Learning, Paper: 293271, Weight: -7.040438262745119e-06\n",
      "Topic: Neural_Networks, Paper: 293271, Weight: -2.112574087847406e-06\n",
      "Topic: Case_Based, Paper: 293271, Weight: 1.6999307753629863e-05\n",
      "Topic: Genetic_Algorithms, Paper: 293271, Weight: 1.267810056985186e-05\n",
      "Topic: Theory, Paper: 293271, Weight: -5.323296101485971e-06\n",
      "Topic: Reinforcement_Learning, Paper: 293271, Weight: -6.070285843064605e-07\n",
      "Topic: Probabilistic_Methods, Paper: 293271, Weight: -1.5762020339952842e-05\n",
      "MIN VAL for 293271 Topic Case_Based WRONG, Expected Reinforcement_Learning\n"
     ]
    }
   ],
   "source": [
    "if \"ipykernel_launcher\" in sys.argv[0]:\n",
    "    # Filter out Jupyter-specific arguments\n",
    "    sys.argv = [sys.argv[0]] + [arg for arg in sys.argv[1:] if not arg.startswith(\"--f=\")]\n",
    "    sys.argv += [\"--dataset\", \"cora\", \"--mode\", \"validation\"]\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser(description=\"GNN-SNN\")\n",
    "    parser.add_argument(\"--dataset\", \"-d\", type=str, choices=[\"cora\", \"citeseer\", \"pubmed\"], required=True)\n",
    "    parser.add_argument(\"--mode\", \"-m\", type=str, choices=[\"validation\", \"test\"], required=True)\n",
    "    parser.add_argument(\"--seed\", \"-s\", type=int, default=0)\n",
    "    parser.add_argument(\"--features\", type=int, choices=[0,1])\n",
    "    parser.add_argument(\"--paper_leak\", type=float, default=100000.0)           # [1., 10., 100., 100000.] \n",
    "    parser.add_argument(\"--paper_threshold\", type=float, default=1.0)          # [0.25, 0.5, 0.75, 1.0, 1.5]\n",
    "    parser.add_argument(\"--paper_tau_minus\", type=float, default=30.)           # [5., 10., 15., 20., 25., 30., 40., 50.]\n",
    "    parser.add_argument(\"--train_ref\", type=float, default=1.0)                 # [1.0, 5.0, 10.0, 100.0, 1000.0]\n",
    "    parser.add_argument(\"--feature_ref\", type=float, default=1.0)                 # [1.0, 5.0, 10.0, 100.0, 1000.0]\n",
    "    parser.add_argument(\"--validation_ref\", type=float, default=1000.0)         # [1.0, 5.0, 10.0, 100.0, 1000.0]\n",
    "\n",
    "    # NOTE: We probably want to keep this parameter the same as the validation parameter above\n",
    "    parser.add_argument(\"--test_ref\", type=float, default=1000.0)               # [1.0, 5.0, 10.0, 100.0, 1000.0]\n",
    "    parser.add_argument(\"--topic_leak\", type=float, default=100000.0)           # [1., 10., 100., 100000.] \n",
    "    parser.add_argument(\"--topic_threshold\", type=float, default=.25)            # [0.25, 0.5, 0.75, 1.0, 1.5]\n",
    "    parser.add_argument(\"--topic_tau_minus\", type=float, default=30.)           # [5., 10., 15., 20., 25., 30., 40., 50.]\n",
    "    parser.add_argument(\"--feature_leak\", type=float, default=100000.0)         # [1., 10., 100., 100000.] \n",
    "    parser.add_argument(\"--feature_threshold\", type=float, default=0.25)       # [0.25, 0.5, 0.75, 1.0, 1.5]\n",
    "    parser.add_argument(\"--feature_tau_minus\", type=float, default=30.0)        # [5., 10., 15., 20., 25., 30., 40., 50.]\n",
    "    parser.add_argument(\"--graph_weight\", type=float, default=10.0)            # [0.5, 1.0, 10.0, 100.0]            \n",
    "    parser.add_argument(\"--graph_delay\", type=float, default=1.0)               # [1., 2., 5., 10., 20.]\n",
    "    parser.add_argument(\"--train_to_topic_weight\", type=float, default=10.0)     # [0.5, 1.0, 10.0, 100.0]\n",
    "    parser.add_argument(\"--train_to_topic_delay\", type=float, default=10.0)      # [1., 2., 5., 10., 20.]\n",
    "    parser.add_argument(\"--validation_to_topic_weight\", type=float, default=0.001)  # [0.001, 0.005, 0.01, 0.05, 0.1]\n",
    "    parser.add_argument(\"--validation_to_topic_delay\", type=float, default=1.0)     # [1., 2., 5., 10., 20.]\n",
    "    parser.add_argument(\"--validation_to_topic_alpha\", type=float, default=1.0)     # [0.5, 1.0, 2.0, 5.0]\n",
    "    parser.add_argument(\"--validation_to_topic_tau_plus\", type=float, default=30.0) # [5., 10., 15., 20., 25., 30., 40., 50.]\n",
    "\n",
    "    # NOTE: We probably want to keep these parameters the same as the validation set parameters\n",
    "    parser.add_argument(\"--test_to_topic_weight\", type=float, default=0.001)    # [0.001, 0.005, 0.01, 0.05, 0.1]\n",
    "    parser.add_argument(\"--test_to_topic_delay\", type=float, default=1.0)       # [1., 2., 5., 10., 20.]\n",
    "    parser.add_argument(\"--test_to_topic_alpha\", type=float, default=1.0)       # [0.5, 1.0, 2.0, 5.0]\n",
    "    parser.add_argument(\"--test_to_topic_tau_plus\", type=float, default=30.0)   # [5., 10., 15., 20., 25., 30., 40., 50.]\n",
    "    parser.add_argument(\"--paper_to_feature_weight\", type=float, default=0.2)   # [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "    parser.add_argument(\"--feature_to_paper_weight\", type=float, default=0.2)   # [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "    parser.add_argument(\"--paper_to_feature_delay\", type=float, default=4.0)    # [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0]\n",
    "    parser.add_argument(\"--paper_to_feature_alpha\", type=float, default=1.0)    # [0.5, 1.0, 2.0, 5.0]\n",
    "    parser.add_argument(\"--paper_to_feature_tau_plus\", type=float, default=30.0)    # [5., 10., 15., 20., 25., 30., 40., 50.]\n",
    "    parser.add_argument(\"--simtime\", type=float, default=5.0)                  # [5., 10., 20., 30., 40., 50. 75., 100., 150., 200.]\n",
    "    parser.add_argument(\"--processes\", type=int, default=4)         # Change this depending on your machine\n",
    "    parser.add_argument(\"--monitors\", type=str, default=\"False\")\n",
    "    parser.add_argument(\"--paper_to_feature_stdp\", type=str, default=\"False\")\n",
    "    parser.add_argument(\"--paper_to_feature_weighted\", type=str, default=\"False\")\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    config = vars(args)\n",
    "\n",
    "    if (config[\"monitors\"] == \"True\" or config[\"monitors\"] == \"true\"):\n",
    "        config[\"monitors\"] = True\n",
    "    else:\n",
    "        config[\"monitors\"] = False\n",
    "\n",
    "    if (config[\"paper_to_feature_stdp\"] == \"True\" or config[\"paper_to_feature_stdp\"] == \"true\"):\n",
    "        config[\"paper_to_feature_stdp\"] = True\n",
    "    else:\n",
    "        config[\"paper_to_feature_stdp\"] = False\n",
    "    \n",
    "    if (config[\"paper_to_feature_weighted\"] == \"True\" or config[\"paper_to_feature_weighted\"] == \"true\"):\n",
    "        config[\"paper_to_feature_weighted\"] = True\n",
    "    else:\n",
    "        config[\"paper_to_feature_weighted\"] = False\n",
    "\n",
    "    np.random.seed(args.seed) \n",
    "    graph = GraphData(args.dataset, config)    \n",
    "    \n",
    "    if (config[\"monitors\"] == True):\n",
    "        for i in range(0, len(graph.validation_papers), 20):\n",
    "            paper = graph.validation_papers[i]\n",
    "            x = [paper, graph, config]\n",
    "            test_paper(x)\n",
    "        sys.exit()\n",
    "\n",
    "     # Spike each of the test papers in simulation\n",
    "    i = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    pool = Pool(args.processes)\n",
    "    if args.mode == \"validation\":\n",
    "        papers = []\n",
    "        #count = 0\n",
    "        #x = 0\n",
    "        for paper in graph.validation_papers:\n",
    "            papers.append([paper, graph, config])\n",
    "            #x = test_paper(papers[count]) \n",
    "            #count += 1\n",
    "        x = pool.map(test_paper, papers)\n",
    "        print(\"Validation Accuracy:\", np.sum(x) / len(graph.validation_papers))\n",
    " \n",
    "    if args.mode == \"test\":\n",
    "        papers = []\n",
    "        for paper in graph.test_papers:\n",
    "            papers.append([paper, graph, config])\n",
    "        x = pool.map(test_paper, papers) \n",
    "        print(\"Testing Accuracy:\", np.sum(x) / len(graph.test_papers))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
