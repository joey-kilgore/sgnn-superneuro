{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from superneuromat.neuromorphicmodel import NeuromorphicModel\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from dataclasses import dataclass, field\n",
    "\n",
    "\n",
    "# from: https://stackoverflow.com/a/21894086/2712730\n",
    "class bidict(dict):\n",
    "    \"\"\"Creates a dictionary that supports reverse lookups via the .inverse attribute.\n",
    "\n",
    "    Args:\n",
    "        dict (dict): The original dictionary.\n",
    "\n",
    "    Properties:\n",
    "        inverse (dict): A dictionary that maps values to keys.\n",
    "    \"\"\"\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(bidict, self).__init__(*args, **kwargs)\n",
    "        self.inverse = {}\n",
    "        for key, value in self.items():\n",
    "            self.inverse.setdefault(value, []).append(key)\n",
    "\n",
    "    def __setitem__(self, key, value):\n",
    "        if key in self:\n",
    "            self.inverse[self[key]].remove(key)\n",
    "        super(bidict, self).__setitem__(key, value)\n",
    "        self.inverse.setdefault(value, []).append(key)\n",
    "\n",
    "    def __delitem__(self, key):\n",
    "        self.inverse.setdefault(self[key], []).remove(key)\n",
    "        if self[key] in self.inverse and not self.inverse[self[key]]:\n",
    "            del self.inverse[self[key]]\n",
    "        super(bidict, self).__delitem__(key)\n",
    "\n",
    "\n",
    "def train_test_split_indices(papers, test_size=0.2, rng=None) -> tuple[np.ndarray[int], np.ndarray[int]]:\n",
    "    \"\"\"\n",
    "    Splits a list of papers into train and test indices.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    papers : iterable of papers with ids\n",
    "        List or dict of papers to split.\n",
    "    test_size : float, optional\n",
    "        if < 1, proportion of papers to reserve for testing, by default 0.2\n",
    "        if > 1, number of papers to reserve for testing\n",
    "    rng : int, optional\n",
    "        Random state for the random number generator, default uses numpy's random\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    train_indices : list of int\n",
    "        List of indices for the training set.\n",
    "    test_indices : list of int\n",
    "        List of indices for the testing set.\n",
    "    \"\"\"\n",
    "    n = papers if isinstance(papers, int) else len(papers)\n",
    "    if test_size < 1:\n",
    "        test_size = int(np.floor(test_size * n))  # number of papers in test\n",
    "    if rng is None:\n",
    "        rng = np.random.default_rng()  # setup rng\n",
    "    elif isinstance(rng, int):\n",
    "        rng = np.random.default_rng(rng)\n",
    "\n",
    "    if isinstance(papers, int):\n",
    "        indices = np.arange(n)  # generate indices\n",
    "    elif isinstance(papers, (list, tuple)):\n",
    "        indices = papers  # assume papers is a list of indices\n",
    "    else:  # assume papers is a dict or mapping of papers with a .values() method\n",
    "        indices = [paper.idx for paper in papers.values()]  # grab indices from dict entries\n",
    "    indices = np.asarray(indices)\n",
    "\n",
    "    # shuffle and split\n",
    "    rng.shuffle(indices)\n",
    "    test_indices = indices[:test_size]\n",
    "    train_indices = indices[test_size:]\n",
    "    return train_indices, test_indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(2024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load CiteSeer dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 3312 papers and skipped 15 broken references.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'AI', 'Agents', 'DB', 'HCI', 'IR', 'ML'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@dataclass\n",
    "class Paper:\n",
    "    idx: str  # Paper ID\n",
    "    label: str  # Paper category/topic\n",
    "    features: tuple[bool, ...] = ()  # binary features\n",
    "    citations: list[str] = field(default_factory=list)  # IDs of papers cited by this paper\n",
    "\n",
    "papers = {}\n",
    "missing = set()\n",
    "\n",
    "# Load in training data\n",
    "content = pd.read_csv(\"data/citeseer/citeseer.content\", sep=\"\\t\", header=None, dtype=object)\n",
    "citations = pd.read_csv(\"data/citeseer/citeseer.cites\", sep=\"\\t\", header=None, dtype=object)\n",
    "\n",
    "labels = set()  # set of unique labels\n",
    "\n",
    "for paper in content.itertuples(index=False):  # create papers from data\n",
    "    idx = paper[0]\n",
    "    features = tuple([int(feature) for feature in paper[1:-1]])  # parse features\n",
    "    papers[idx] = Paper(idx, paper[-1], features)  # create paper object\n",
    "    labels.add(paper[-1])  # label is the last column. add to set of labels\n",
    "\n",
    "for paper_idx, citation in citations.itertuples(index=False):\n",
    "    try:  # parse citations\n",
    "        if citation not in papers:\n",
    "            missing.add(citation)  # if citation is missing, add to missing list\n",
    "            continue\n",
    "        papers[paper_idx].citations.append(citation)\n",
    "    except KeyError:\n",
    "        missing.add(paper_idx)  # if paper is missing, add to missing list\n",
    "\n",
    "print(f\"Loaded {len(papers)} papers and skipped {len(missing)} broken references.\")\n",
    "\n",
    "labels  # show labels set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create train / test split  \n",
    "Create two lists of IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_idxs, test_idxs = train_test_split_indices(papers, test_size=0.2, rng=rng)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize our model\n",
    "model = NeuromorphicModel()\n",
    "\n",
    "# Create our output neurons, set threshold very high so that we control when they spike during training.\n",
    "# dict mapping {category: neuron_id}\n",
    "lbl_threshold = 2.0\n",
    "strong_connection = 5.0\n",
    "weak_connection = 1.0\n",
    "unknown_connection = 0.00001\n",
    "\n",
    "lbl_neurons = bidict({label: model.create_neuron(threshold=lbl_threshold).idx for label in labels})\n",
    "\n",
    "# Create our input neurons, one for each pixel of the image resolution.\n",
    "paper_neurons = {}  # dict mapping {paper_id: neuron_id}\n",
    "\n",
    "# make a neuron for each paper\n",
    "for paper in papers.values():\n",
    "    paper_neurons[paper.idx] = neuron_id = model.create_neuron().idx\n",
    "\n",
    "    if paper.idx in train_idxs:\n",
    "        # Make an explicitly STRONG synapse connecting the input to the output\n",
    "        output_id = lbl_neurons[paper.label]  # (training paper to topic)\n",
    "        model.create_synapse(neuron_id, output_id, weight=strong_connection, stdp_enabled=False, delay=1)\n",
    "        model.create_synapse(output_id, neuron_id, weight=strong_connection, stdp_enabled=False, delay=1)\n",
    "    else:  # test set, connect this input neuron to all output neurons\n",
    "        # Connect our input neuron to output neurons (test paper to topic)\n",
    "        for output_id in lbl_neurons.values():\n",
    "            # Randomize initial weight\n",
    "            weight1 = (rng.uniform(-1, 1)) * unknown_connection\n",
    "            weight2 = (rng.uniform(-1, 1)) * unknown_connection\n",
    "            # Make a synapse connecting the input to the output\n",
    "            model.create_synapse(neuron_id, output_id, weight=weight1, stdp_enabled=True, delay=1)\n",
    "            model.create_synapse(output_id, neuron_id, weight=weight2, stdp_enabled=True, delay=1)\n",
    "\n",
    "# connect papers by their citations (paper to paper connections)\n",
    "for paper in papers.values():\n",
    "    for citation in paper.citations:\n",
    "        model.create_synapse(paper_neurons[paper.idx], paper_neurons[citation], weight=weak_connection, stdp_enabled=True, delay=1)\n",
    "        model.create_synapse(paper_neurons[citation], paper_neurons[paper.idx], weight=weak_connection, stdp_enabled=True, delay=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### loop through our dataset and add spikes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestep = 0\n",
    "train_idxs_augmented = np.append(train_idxs, train_idxs)\n",
    "for idx in train_idxs_augmented:\n",
    "    paper = papers[idx]  # get paper by id\n",
    "    # Add a spike that will exceed the threshold for the respective label neuron\n",
    "    model.add_spike(timestep + 1, lbl_neurons[paper.label], strong_connection)\n",
    "    # Add spikes to the paper\n",
    "    model.add_spike(timestep, paper_neurons[paper.idx], strong_connection)\n",
    "    timestep += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup the model and perform a training pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your model has 3318 neurons and 22674 synapses.\n",
      "5300 time steps will be simulated.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e0028b2085f49ac9c8c7bd5b98dead0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Set up our stdp, only one timestep because we only want it looking at the results\n",
    "# of what our input layer does to our output layer\n",
    "model.stdp_setup(Apos=[1e-4, 1e-4], Aneg=[-1e-5, -1e-5], negative_update=True, positive_update=True)\n",
    "# model.setup()\n",
    "\n",
    "print(f\"Your model has {model.num_neurons} neurons and {model.num_synapses} synapses.\\n{timestep} time steps will be simulated.\")\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Simulate\n",
    "with tqdm(total=timestep) as pbar:\n",
    "    model.simulate(time_steps=timestep, callback=lambda _s, _t, _n: pbar.update())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def blank_copy(model):\n",
    "    new = NeuromorphicModel()\n",
    "    new.neuron_states = model.neuron_states.copy()\n",
    "    new.neuron_refractory_periods_state = model.neuron_refractory_periods_state.copy()\n",
    "    new.neuron_thresholds = model.neuron_thresholds.copy()\n",
    "    new.neuron_leaks = model.neuron_leaks.copy()\n",
    "    new.neuron_reset_states = model.neuron_reset_states.copy()\n",
    "    new.neuron_refractory_periods = model.neuron_refractory_periods.copy()\n",
    "    new.pre_synaptic_neuron_ids = model.pre_synaptic_neuron_ids.copy()\n",
    "    new.post_synaptic_neuron_ids = model.post_synaptic_neuron_ids.copy()\n",
    "    new.synaptic_weights = model.synaptic_weights.copy()\n",
    "    new.synaptic_delays = model.synaptic_delays.copy()\n",
    "    new.enable_stdp = model.enable_stdp.copy()\n",
    "    return new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 10022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed: 10030\n",
      "Paper: terrillon98automatic\tCategory: ML\n",
      "Spiked categories:\n",
      "\t0\tML\n",
      "\t1\tAgents\n",
      "\t2\tHCI\n",
      "\t3\tAI\n",
      "\t4\tIR\n",
      "\t5\tDB\n",
      "==========\n",
      "0\t 0.00000\tML\n",
      "1\t 0.00000\tAgents\n",
      "2\t 0.00000\tHCI\n",
      "3\t 0.00000\tAI\n",
      "4\t 0.00000\tIR\n",
      "5\t 0.00000\tDB\n",
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "[6, 6, 6, 6, 6, 6]\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 1. 0. 0. 0.]\n",
      "[1. 0. 1. 0. 0. 0.]\n",
      "[1. 1. 1. 1. 1. 1.]\n",
      "[1. 1. 1. 1. 1. 1.]\n",
      "[1. 1. 1. 1. 1. 1.]\n",
      "[1. 1. 1. 1. 1. 1.]\n",
      "[1. 1. 1. 1. 1. 1.]\n",
      "[1. 1. 1. 1. 1. 1.]\n",
      "[1. 1. 1. 1. 1. 1.]\n",
      "[1. 1. 1. 1. 1. 1.]\n",
      "[1. 1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "seed += 1\n",
    "print(f\"Seed: {seed}\")\n",
    "rng = np.random.default_rng(seed)\n",
    "model2 = blank_copy(model)\n",
    "model2.stdp = False\n",
    "nlabels = len(labels)\n",
    "# model2.enable_stdp = np.zeros(model.num_neurons)\n",
    "\n",
    "model2.neuron_thresholds[:nlabels] = [99999] * nlabels\n",
    "\n",
    "# paper = random.choice(list(papers.values()))  # pick a random paper\n",
    "# paper = papers[random.choice(train_idxs)]  # pick a random paper from training set\n",
    "paper = papers[rng.choice(test_idxs)]  # pick a random paper from testing set\n",
    "\n",
    "print(f\"Paper: {paper.idx}\\tCategory: {paper.label}\")\n",
    "\n",
    "spike_n = 2\n",
    "\n",
    "for t in range(spike_n):\n",
    "    model2.add_spike(t, paper_neurons[paper.idx], strong_connection)\n",
    "\n",
    "model2.simulate(spike_n + 1)\n",
    "\n",
    "output_spikes = np.sum(model2.ispikes[-(spike_n + 1):, :nlabels], axis=0)\n",
    "\n",
    "spiked_ids = {idx: lbl_neurons.inverse[idx][0]\n",
    "              for idx, spiked in enumerate(output_spikes) if spiked}\n",
    "if spiked_ids:\n",
    "    print(f\"Spiked categories:\")\n",
    "    for idx, category in spiked_ids.items():\n",
    "        print(f\"\\t{idx}\\t{category}\")\n",
    "else:\n",
    "    print(\"No category spiked\")\n",
    "\n",
    "print('=' * 10)\n",
    "\n",
    "lbl_by_threshold = sorted((enumerate(model2.neuron_states[:nlabels])), key=lambda x: x[1], reverse=True)\n",
    "for i, v in lbl_by_threshold:\n",
    "    category = lbl_neurons.inverse[i][0]\n",
    "    print(f\"{i}\\t{v: 5.5f}\\t{category}\")\n",
    "print(model2.neuron_states[:nlabels])\n",
    "print(model2.neuron_thresholds[:nlabels])\n",
    "for ts in model2.spike_train:\n",
    "    print(ts[:nlabels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_threshold = 6.058\n",
    "\n",
    "def evaluate_paper(paper_idx):\n",
    "    model_temp = blank_copy(model)\n",
    "    model_temp.stdp = False\n",
    "    model_temp.neuron_thresholds[:nlabels] = [test_threshold] * nlabels\n",
    "    for t in range(2):\n",
    "        model_temp.add_spike(t, paper_neurons[paper_idx], strong_connection)\n",
    "    model_temp.simulate(1 + 2)\n",
    "    output_spikes = np.sum(model_temp.ispikes[-(1):, :nlabels], axis=0)\n",
    "    return output_spikes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f97fe303e6842c6be5fb1979d79ee92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/662 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# single-threaded (slow, but better for debugging)\n",
    "results = [evaluate_paper(paper_idx) for paper_idx in tqdm(test_idxs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tp: 491 / 589 / 662 | Perfect: 362 / 589 / 662 (correct / attempted / total)\n",
      "tp:        0.741692 | Perfect: 0.546828 | F1:        0.740015\n",
      "Precision: 0.643512 | Recall:  0.870567 | Accuracy:  0.587321\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NeuromorphicModel' object has no attribute 'stdp_Apos'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 21\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtp:        \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtp\u001b[38;5;250m \u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;250m \u001b[39mtotal\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m>.6f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | Perfect: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcorrect\u001b[38;5;250m \u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;250m \u001b[39mtotal\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m>.6f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | F1:        \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtp\u001b[38;5;250m \u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;250m \u001b[39m(tp\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m(\u001b[38;5;241m0.5\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;250m \u001b[39m(fp\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39mfn)))\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m>.6f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPrecision: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtp\u001b[38;5;250m \u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;250m \u001b[39m(tp\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39mfp)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m>.6f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | Recall:  \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtp\u001b[38;5;250m \u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;250m \u001b[39m(tp\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39mfn)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m>.6f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | Accuracy:  \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m(tp\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39mtn)\u001b[38;5;250m \u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;250m \u001b[39m(tp\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39mtn\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39mfp\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39mfn)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m>.6f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     20\u001b[0m tabulate([[correct, tp, fp, tn, fn, n_guesses, total,\n\u001b[1;32m---> 21\u001b[0m            \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstdp_Apos\u001b[49m\u001b[38;5;241m.\u001b[39mtolist(), model\u001b[38;5;241m.\u001b[39mstdp_Aneg\u001b[38;5;241m.\u001b[39mtolist(), lbl_threshold,\n\u001b[0;32m     22\u001b[0m            strong_connection, weak_connection, unknown_connection, test_threshold]], tablefmt\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhtml\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NeuromorphicModel' object has no attribute 'stdp_Apos'"
     ]
    }
   ],
   "source": [
    "from tabulate import tabulate\n",
    "\n",
    "tp, fp, tn, fn = 0, 0, 0, 0\n",
    "correct = 0\n",
    "total = len(test_idxs)\n",
    "\n",
    "for actual_idx, spikes in zip(test_idxs, results):\n",
    "    correct_label = papers[actual_idx].label\n",
    "    guesses = {lbl_neurons.inverse[idx][0] for idx, spiked in enumerate(spikes) if spiked}\n",
    "    tp += correct_label in guesses\n",
    "    fn += not bool(guesses)  # +1 false negative if no guesses\n",
    "    fp += len([x for x in guesses if x != correct_label])\n",
    "    correct += correct_label in guesses and len(guesses) == 1\n",
    "\n",
    "n_guesses = total - fn\n",
    "print(f\"tp: {tp} / {n_guesses} / {total} | Perfect: {correct} / {n_guesses} / {total} (correct / attempted / total)\")\n",
    "print(f\"tp:        {tp / total:>.6f} | Perfect: {correct / total:>.6f} | F1:        {tp / (tp + (0.5 * (fp + fn))):>.6f}\")\n",
    "print(f\"Precision: {tp / (tp + fp):>.6f} | Recall:  {tp / (tp + fn):>.6f} | Accuracy:  {(tp + tn) / (tp + tn + fp + fn):>.6f}\")\n",
    "\n",
    "tabulate([[correct, tp, fp, tn, fn, n_guesses, total,\n",
    "           model.stdp_Apos.tolist(), model.stdp_Aneg.tolist(), lbl_threshold,\n",
    "           strong_connection, weak_connection, unknown_connection, test_threshold]], tablefmt=\"html\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'HCI': 0, 'AI': 1, 'Agents': 2, 'ML': 3, 'DB': 4, 'IR': 5}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lbl_neurons"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sngnn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
