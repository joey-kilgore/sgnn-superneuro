{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6eabfc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from superneuromat.neuromorphicmodel import NeuromorphicModel\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c163bd2-d9f3-4358-bca6-49202017cf2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphData():\n",
    "    def __init__(self, name, config):\n",
    "        self.name = name\n",
    "        self.config = config\n",
    "        self.paper_to_topic = {} # maps the paper ID in the dataset to its topic ID\n",
    "        self.index_to_paper = []    # creates an index for each paper\n",
    "        self.topics = []            # the list of topics \n",
    "        self.train_papers = []\n",
    "        self.validation_papers = []\n",
    "        self.test_papers = []\n",
    "        self.load_topics()\n",
    "        self.train_val_test_split()\n",
    "        self.load_features()\n",
    "        self.load_graph()\n",
    "\n",
    "\n",
    "    def load_topics(self):\n",
    "        if (self.name == \"cora\"):\n",
    "            f = open(\"data/Cora/group-edges.csv\", 'r')\n",
    "            lines = f.readlines()\n",
    "            f.close()\n",
    "\n",
    "            for line in lines:\n",
    "                fields = line.strip().split(\",\")\n",
    "                if (fields[1] not in self.topics):\n",
    "                    self.topics.append(fields[1])\n",
    "                self.paper_to_topic[fields[0]] = fields[1]\n",
    "                self.index_to_paper.append(fields[0])\n",
    "        elif (self.name == \"citeseer\"):\n",
    "            f = open(\"data/citeseer/citeseer.content\", 'r')\n",
    "            lines = f.readlines()\n",
    "            f.close()\n",
    "\n",
    "            for line in lines:\n",
    "                fields = line.strip().split()\n",
    "                if (fields[-1] not in self.topics):\n",
    "                    self.topics.append(fields[-1])\n",
    "                #print(fields[0])\n",
    "                self.paper_to_topic[fields[0]] = fields[-1]\n",
    "                self.index_to_paper.append(fields[0])   \n",
    "        elif (self.name == \"pubmed\"):\n",
    "            f = open(\"data/Pubmed-Diabetes/data/Pubmed-Diabetes.NODE.paper.tab\", 'r')\n",
    "            lines = f.readlines()\n",
    "            f.close()\n",
    "            lines = lines[2:]\n",
    "        \n",
    "            for line in lines:\n",
    "                fields = line.strip().split()\n",
    "                if (fields[1] not in self.topics):\n",
    "                    self.topics.append(fields[1])\n",
    "                self.paper_to_topic[\"paper:\"+fields[0]] = fields[1]\n",
    "                self.index_to_paper.append(\"paper:\"+fields[0])\n",
    "\n",
    "\n",
    "\n",
    "    def load_features(self):\n",
    "        self.features = {} # keyed on paper ID, value is the feature vector\n",
    "        if (self.name == \"cora\"):\n",
    "            f = open(\"data/Cora/cora/cora.content\", 'r')\n",
    "            lines = f.readlines()\n",
    "            f.close()\n",
    "            \n",
    "            for line in lines:\n",
    "                fields = line.strip().split()\n",
    "                paper_id = fields[0]\n",
    "                feature = [int(x) for x in fields[1:-1]]\n",
    "                self.features[paper_id] = feature\n",
    "                self.num_features = len(feature)\n",
    "            print(\"NUM PAPERS WITH FEATURES: \", len(self.features.keys()))\n",
    "        elif (self.name == \"citeseer\"):\n",
    "            f = open(\"data/citeseer/citeseer.content\", 'r')\n",
    "            lines = f.readlines()\n",
    "            f.close()\n",
    "            for line in lines:\n",
    "                fields = line.strip().split()\n",
    "                paper_id = fields[0]\n",
    "                feature = [int(x) for x in fields[1:-1]]\n",
    "                self.features[paper_id] = feature\n",
    "                self.num_features = len(feature)\n",
    "            print(\"NUM PAPERS WITH FEATURES: \", len(self.features.keys()))\n",
    "        elif (self.name == \"pubmed\"):\n",
    "            f = open(\"data/Pubmed-Diabetes/data/Pubmed-Diabetes.NODE.paper.tab\", 'r')\n",
    "            lines = f.readlines()\n",
    "            f.close()\n",
    "            feature_line = lines[1]\n",
    "            fields = feature_line.split()\n",
    "            fields = fields[1:-1]\n",
    "            all_features = {}\n",
    "            for i in range(len(fields)):\n",
    "                feat = fields[i].split(':')[1]\n",
    "                all_features[feat] = i\n",
    "\n",
    "            self.num_features = len(all_features.keys())\n",
    "            lines = lines[2:]\n",
    "            for line in lines:\n",
    "                feature = [0]*self.num_features\n",
    "                fields = line.split()\n",
    "                paper_id = \"paper:\" + fields[0]\n",
    "                xfields = fields[-1].split('=')\n",
    "                xfields = xfields[1].split(',')\n",
    "                for x in xfields:\n",
    "                    feature[all_features[x]] = 1\n",
    "                self.features[paper_id] = feature\n",
    "\n",
    "        self.paper_to_features = {} # keyed on paper ID, value is the number of features it has\n",
    "        self.feature_to_papers = {} # keyed on feature ID, value is the number of papers that have that feature\n",
    "        for p in self.features.keys():\n",
    "            self.paper_to_features[p] = np.sum(self.features[p])\n",
    "            for i in range(len(self.features[p])):\n",
    "                if (i not in self.feature_to_papers.keys()):\n",
    "                    self.feature_to_papers[i] = 0\n",
    "                self.feature_to_papers[i] += self.features[p][i]\n",
    "\n",
    "\n",
    "    def load_graph(self):\n",
    "        if (self.name == \"cora\"):\n",
    "            self.graph = nx.read_edgelist(\"data/Cora/edges.csv\", delimiter=\",\")\n",
    "\n",
    "        elif (self.name == \"citeseer\"):\n",
    "            self.graph = nx.read_edgelist(\"data/citeseer/citeseer.cites\")\n",
    "\n",
    "        elif (self.name == \"pubmed\"):\n",
    "            self.graph = nx.read_edgelist(\"data/Pubmed-Diabetes/data/edge_list.csv\", delimiter=\",\")\n",
    "\n",
    "\n",
    "    def train_val_test_split(self):\n",
    "        np.random.seed(432)\n",
    "        train_papers = []\n",
    "        test_papers = []\n",
    "        validation_papers = []\n",
    "        check_breakdown = {}\n",
    "        \n",
    "        for k in self.topics:\n",
    "            check_breakdown[k] = 0\n",
    "        \n",
    "        while (len(train_papers) < len(self.topics)*20):\n",
    "            index = np.random.randint(len(self.index_to_paper))\n",
    "            topic = self.paper_to_topic[self.index_to_paper[index]]\n",
    "            if (check_breakdown[topic] < 20 and self.index_to_paper[index] not in train_papers):\n",
    "                train_papers.append(self.index_to_paper[index])\n",
    "                check_breakdown[topic] += 1\n",
    "        \n",
    "        while (len(validation_papers) < len(self.topics)*20):\n",
    "            index = np.random.randint(len(self.index_to_paper))\n",
    "            topic = self.paper_to_topic[self.index_to_paper[index]]\n",
    "            if (check_breakdown[topic] < 40 and self.index_to_paper[index] not in train_papers and self.index_to_paper[index] not in validation_papers):\n",
    "                validation_papers.append(self.index_to_paper[index])\n",
    "                check_breakdown[topic] += 1\n",
    "        \n",
    "        for i in range(len(self.index_to_paper)):\n",
    "            if (self.index_to_paper[i] not in train_papers and self.index_to_paper[i] not in validation_papers and self.index_to_paper[i] in self.paper_to_topic.keys()):\n",
    "                test_papers.append(self.index_to_paper[i])\n",
    "\n",
    "        self.train_papers = train_papers\n",
    "        self.test_papers = test_papers\n",
    "        self.validation_papers = validation_papers \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f675111",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_network(graph):\n",
    "    # Read paper to paper edge list\n",
    "    topic_neurons = {}\n",
    "    # Create paper neurons\n",
    "    paper_neurons = {}\n",
    "    i = 0\n",
    "    for node in graph.graph.nodes:\n",
    "        if (node not in graph.paper_to_topic.keys()):\n",
    "            continue\n",
    "        neuron = model.create_neuron(threshold=1, leak=100000, refractory_period=1000)\n",
    "        paper_neurons[node] = neuron\n",
    "        \n",
    "        \n",
    "    for t in graph.topics:\n",
    "        neuron = model.create_neuron(threshold=1, leak=100000, refractory_period=1000)\n",
    "        topic_neurons[t] = neuron\n",
    "\n",
    "    print(i)\n",
    "    for edge in graph.graph.edges:\n",
    "        if (edge[0] not in graph.paper_to_topic.keys() or edge[1] not in graph.paper_to_topic.keys()):\n",
    "            continue\n",
    "        pre = paper_neurons[edge[0]]\n",
    "        post = paper_neurons[edge[1]]\n",
    "        graph_weight = 100.0\n",
    "        graph_delay = 1\n",
    "        model.create_synapse(pre, post, weight=graph_weight, delay=graph_delay)\n",
    "        model.create_synapse(post, pre, weight=graph_weight, delay=graph_delay)\n",
    "\n",
    "    for paper in graph.train_papers:\n",
    "        paper_neuron = paper_neurons[paper]\n",
    "        topic_neuron = topic_neurons[graph.paper_to_topic[paper]]\n",
    "        train_to_topic_w = 1.0\n",
    "        train_to_topic_d = 1\n",
    "        model.create_synapse(paper_neuron, topic_neuron, weight=train_to_topic_w, delay=train_to_topic_d)\n",
    "        model.create_synapse(topic_neuron, paper_neuron, weight=train_to_topic_w, delay=train_to_topic_d)\n",
    "\n",
    "    for paper in graph.validation_papers:\n",
    "        for topic in graph.topics:\n",
    "            paper_neuron = paper_neurons[paper]\n",
    "            topic_neuron = topic_neurons[topic]\n",
    "            validation_to_topic_w = 0.001\n",
    "            validation_to_topic_d = 1\n",
    "            model.create_synapse(paper_neuron, topic_neuron, enable_stdp=True, weight=validation_to_topic_w, delay=validation_to_topic_d)\n",
    "            model.create_synapse(topic_neuron, paper_neuron, enable_stdp=True, weight=validation_to_topic_w, delay=validation_to_topic_d)\n",
    "\n",
    "    for paper in graph.test_papers:\n",
    "        for topic in graph.topics:\n",
    "            paper_neuron = paper_neurons[paper]\n",
    "            topic_neuron = topic_neurons[topic]\n",
    "            test_to_topic_w = 0.001\n",
    "            test_to_topic_d = 1\n",
    "            model.create_synapse(paper_neuron, topic_neuron, enable_stdp=True, weight=test_to_topic_w, delay=test_to_topic_d)\n",
    "            model.create_synapse(topic_neuron, paper_neuron, enable_stdp=True, weight=test_to_topic_w, delay=test_to_topic_d)\n",
    "\n",
    "\n",
    "    model.stdp_setup(time_steps=1, Apos=[0.001], Aneg=[0.000001], negative_update=True, positive_update=True)\n",
    "    \n",
    "    return paper_neurons, topic_neurons\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fd0790e3-279c-4e97-b3e8-60a83fa13dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_paper(id, graph):\n",
    "    paper_neurons, topic_neurons = load_network(graph)\n",
    "    model.add_spike(0, paper_neurons[paper], 100.0)\n",
    "    model.setup()\n",
    "    model.simulate(time_steps=7)\n",
    "    model.print_spike_train()\n",
    "    topic_ids = {} \n",
    "    times = []\n",
    "    min_weight = 1000\n",
    "    \n",
    "    paper\n",
    "\n",
    "    # Analyze the weights between the test paper neuron and topic neurons\n",
    "    min_weight = float('inf')\n",
    "    min_topic = None\n",
    "    for topic in topic_neurons:\n",
    "        # Find the synapse from topic neuron to test paper neuron\n",
    "        synapse_indices = [\n",
    "            i for i, (pre, post) in enumerate(zip(model.pre_synaptic_neuron_ids, model.post_synaptic_neuron_ids))\n",
    "            if pre == topic_id and post == test_paper_id\n",
    "        ]\n",
    "        if synapse_indices:\n",
    "            idx = synapse_indices[0]\n",
    "            weight = model.synaptic_weights[idx]\n",
    "            print(f\"Topic: {topic}, Paper: {paper}, Weight: {weight}\")\n",
    "            if weight < min_weight:\n",
    "                min_weight = weight\n",
    "                min_topic = topic\n",
    "                \n",
    "                \n",
    "                #need to setup a list of zipped synapses so I can find the ids and get the weight\n",
    "                #check above where I setup synapses to get the lists to iterate over"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f4077598-7807-424f-9e54-b652a79b15c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NUM PAPERS WITH FEATURES:  3312\n"
     ]
    }
   ],
   "source": [
    "graph = GraphData(\"citeseer\", 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1de5b546",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Time: 0, Spikes: [0 0 0 ... 0 0 0]\n",
      "Time: 1, Spikes: [0 0 0 ... 0 0 0]\n",
      "Time: 2, Spikes: [0 0 0 ... 0 0 0]\n",
      "Time: 3, Spikes: [0 0 0 ... 0 0 0]\n",
      "Time: 4, Spikes: [0 0 0 ... 0 0 0]\n",
      "Time: 5, Spikes: [0 0 0 ... 0 0 0]\n",
      "Time: 6, Spikes: [0 0 0 ... 0 0 0]\n",
      "Agents\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = NeuromorphicModel()\n",
    "for paper in graph.validation_papers:\n",
    "    test_paper(paper, graph)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f966d196-68df-45cf-83b6-a524f4a01a44",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
